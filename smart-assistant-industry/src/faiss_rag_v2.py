#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simplified RAG System V2 - Optimized for University Rules & Regulations
Removes academic level detection, adds structure-aware retrieval
Uses enhanced metadata from structure-aware chunking
"""

import os
import json
import time
import re
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# Import required packages
try:
    from sentence_transformers import SentenceTransformer
    import faiss
    from openai import OpenAI
    print("âœ… All required packages imported successfully")
except ImportError as e:
    print(f"âŒ Missing required package: {e}")
    print("Please install: pip install sentence-transformers faiss-cpu python-dotenv openai")
    exit(1)


@dataclass
class RAGResponse:
    """Structured response from RAG system"""
    query: str
    answer: str
    answer_plain_text: str
    sources: List[Dict[str, Any]]
    confidence: float
    generation_time: float
    retrieval_time: float
    total_time: float
    context_used: str
    category: str
    query_type: str
    related_articles: List[int]


class UniversityRulesRAG:
    """Simplified RAG system for university rules and regulations"""
    
    def __init__(self, 
                 index_dir: str = "./index",
                 env_path: str = "../.env"):
        """Initialize the RAG system"""
        
        # Load environment variables
        load_dotenv(env_path)

        # Get API key from environment or fallback
        api_key = os.getenv("OPENROUTER_API_KEY") or "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"
        if not api_key:
            raise ValueError("OPENROUTER_API_KEY not found in environment and no fallback provided")

        print("ğŸ”‘ Loaded API key starts with:", api_key[:15])

        self.llm_model = os.getenv("LLM_MODEL", "openai/gpt-4o-mini")
        
        # Initialize OpenRouter client with required headers
        try:
            self.openai_client = OpenAI(
                base_url="https://openrouter.ai/api/v1",
                api_key=api_key,
                default_headers={
                    "HTTP-Referer": "https://hadisadoghiyazdi1971.github.io",
                    "X-Title": "FUM University Rules RAG Chatbot",
                }
            )
            # Quick test to confirm authentication
            _ = self.openai_client.models.list()
            print(f"âœ… Connected to OpenRouter with model: {self.llm_model}")
        except Exception as e:
            print(f"âŒ Error connecting to OpenRouter: {e}")
            raise

        self.index_dir = Path(index_dir)
        
        # Load index summary
        summary_path = self.index_dir / "index_summary.json"
        if not summary_path.exists():
            raise FileNotFoundError(f"Index summary not found: {summary_path}")
        
        with open(summary_path, 'r', encoding='utf-8') as f:
            self.index_summary = json.load(f)
        
        # Get embedding model from summary
        self.embedding_model_name = self.index_summary.get(
            'embedding_model',
            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
        )
        
        # Initialize embedding model
        print(f"ğŸ¤– Loading embedding model: {self.embedding_model_name}")
        self.embedding_model = SentenceTransformer(self.embedding_model_name, device="cpu")
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        
        # Load FAISS index
        index_path = self.index_dir / "faiss_index.bin"
        if not index_path.exists():
            raise FileNotFoundError(f"FAISS index not found: {index_path}")
        
        print(f"ğŸ“‚ Loading FAISS index from: {index_path}")
        self.faiss_index = faiss.read_index(str(index_path))
        print(f"âœ… FAISS index loaded with {self.faiss_index.ntotal} vectors")
        
        # Load metadata
        metadata_path = self.index_dir / "metadata.json"
        if not metadata_path.exists():
            raise FileNotFoundError(f"Metadata not found: {metadata_path}")
        
        print(f"ğŸ“‚ Loading metadata from: {metadata_path}")
        with open(metadata_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        print(f"âœ… Loaded metadata for {len(self.metadata)} chunks")
        
        # Build article index for quick lookup
        self._build_article_index()
        
        # RAG configuration
        self.max_context_length = 3000
        self.max_sources = 5
        
        # Query type patterns
        self.query_patterns = {
            'definitional': [r'Ú†ÛŒØ³Øª', r'ØªØ¹Ø±ÛŒÙ', r'Ù…Ù†Ø¸ÙˆØ± Ø§Ø²', r'Ù…ÙÙ‡ÙˆÙ…', r'ÛŒØ¹Ù†ÛŒ Ú†Ù‡', r'what is', r'define', r'meaning'],
            'procedural': [r'Ú†Ú¯ÙˆÙ†Ù‡', r'Ú†Ø·ÙˆØ±', r'Ù†Ø­ÙˆÙ‡', r'Ø±ÙˆØ´', r'Ù…Ø±Ø§Ø­Ù„', r'Ú¯Ø§Ù…', r'ÙØ±Ø¢ÛŒÙ†Ø¯', r'how to', r'procedure', r'process', r'steps'],
            'eligibility': [r'Ø¢ÛŒØ§ Ù…ÛŒ\s*ØªÙˆØ§Ù†Ù…', r'Ø¢ÛŒØ§ Ù…Ø¬Ø§Ø²', r'Ø´Ø±Ø§ÛŒØ·', r'Ø¶ÙˆØ§Ø¨Ø·', r'Ø§Ù„Ø²Ø§Ù…Ø§Øª', r'can I', r'may I', r'requirements', r'conditions', r'eligible'],
            'numerical': [r'Ú†Ù†Ø¯', r'Ú†Ù‚Ø¯Ø±', r'Ù…ÛŒØ²Ø§Ù†', r'Ø¯Ø±ØµØ¯', r'ØªØ¹Ø¯Ø§Ø¯', r'Ù…Ø¨Ù„Øº', r'how much', r'how many', r'amount', r'percentage'],
            'exception': [r'ØªØ¨ØµØ±Ù‡', r'Ø§Ø³ØªØ«Ù†Ø§', r'Ù…Ú¯Ø±', r'Ø¨Ù‡ Ø¬Ø²', r'Ù…ÙˆØ§Ø±Ø¯ Ø®Ø§Øµ', r'exception', r'special case', r'note'],
            'timeline': [r'Ù…Ù‡Ù„Øª', r'Ø²Ù…Ø§Ù†', r'ØªØ§Ø±ÛŒØ®', r'Ù…Ø¯Øª', r'deadline', r'when', r'time']
        }
        
        print("âœ… Simplified RAG system initialized")

    # ... Ø§Ø¯Ø§Ù…Ù‡ ØªÙ…Ø§Ù… Ù…ØªØ¯Ù‡Ø§ÛŒ Ú©Ù„Ø§Ø³ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± ...


    
    def _build_article_index(self):
        """Build an index of chunks by article number for quick lookup"""
        self.article_to_chunks = {}
        
        for i, meta in enumerate(self.metadata):
            for article_num in meta.get('article_numbers', []):
                if article_num not in self.article_to_chunks:
                    self.article_to_chunks[article_num] = []
                self.article_to_chunks[article_num].append(i)
        
        print(f"ğŸ“‹ Built article index: {len(self.article_to_chunks)} unique articles")
    
    def detect_query_type(self, query: str) -> str:
        """Detect the type of query to optimize retrieval"""
        query_lower = query.lower()
        
        scores = {qtype: 0 for qtype in self.query_patterns.keys()}
        
        for qtype, patterns in self.query_patterns.items():
            for pattern in patterns:
                if re.search(pattern, query_lower):
                    scores[qtype] += 1
        
        # Return type with highest score, or 'general' if no match
        max_score = max(scores.values())
        if max_score == 0:
            return 'general'
        
        return max(scores, key=scores.get)
    
    def extract_article_mentions(self, query: str) -> List[int]:
        """Extract article numbers mentioned in the query"""
        article_numbers = []
        
        # Persian patterns
        patterns = [
            r'Ù…Ø§Ø¯Ù‡\s+(\d+)',
            r'Ø¨Ù†Ø¯\s+(\d+)',
            r'ØªØ¨ØµØ±Ù‡\s+(\d+)'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, query)
            for match in matches:
                try:
                    num = int(match.group(1))
                    if num not in article_numbers:
                        article_numbers.append(num)
                except ValueError:
                    pass
        
        return sorted(article_numbers)
    
    def retrieve_relevant_context(
        self, 
        query: str, 
        top_k: int = 5,
        query_type: str = 'general'
    ) -> Tuple[List[Dict[str, Any]], float, List[int]]:
        """Retrieve relevant context with structure-aware filtering"""
        
        retrieval_start = time.time()
        
        # Extract mentioned articles
        mentioned_articles = self.extract_article_mentions(query)
        
        # Embed the query
        query_embedding = self.embedding_model.encode([query])[0].astype('float32')
        
        # Search in FAISS index - get more results for filtering
        k_search = min(top_k * 4, self.faiss_index.ntotal)
        distances, indices = self.faiss_index.search(
            query_embedding.reshape(1, -1), 
            k_search
        )
        
        # Process results
        candidate_sources = []
        related_articles = set(mentioned_articles)
        
        for idx, (distance, index) in enumerate(zip(distances[0], indices[0])):
            if index == -1 or index >= len(self.metadata):
                continue
            
            meta = self.metadata[index]
            
            # Convert L2 distance to similarity score
            similarity_score = 1.0 / (1.0 + distance)
            
            # Extract document info
            source_file = meta.get('source_file', '')
            document_id = source_file.replace('.txt', '') if source_file.endswith('.txt') else source_file
            
            # Boost score based on query type and chunk type
            boost = 1.0
            chunk_type = meta.get('chunk_type', 'general')
            
            if query_type == 'definitional' and chunk_type == 'definitions':
                boost = 1.3
            elif query_type == 'procedural' and chunk_type == 'article':
                boost = 1.2
            elif query_type == 'numerical' and meta.get('has_table', False):
                boost = 1.3
            elif query_type == 'exception' and meta.get('note_numbers'):
                boost = 1.25
            
            # Boost if article is mentioned in query
            article_numbers = meta.get('article_numbers', [])
            if any(art in mentioned_articles for art in article_numbers):
                boost *= 1.5
            
            adjusted_score = similarity_score * boost
            
            source_data = {
                "score": float(adjusted_score),
                "original_score": float(similarity_score),
                "content": meta.get('chunk_text', ''),
                "document_title": meta.get('document_title', document_id),
                "document_file": source_file,
                "category": meta.get('chunk_type', 'general'),
                "chunk_type": chunk_type,
                "document_section": meta.get('document_section', ''),
                "article_numbers": article_numbers,
                "note_numbers": meta.get('note_numbers', []),
                "has_table": meta.get('has_table', False),
                "has_list": meta.get('has_list', False),
                "document_id": document_id,
                "chunk_id": meta.get('chunk_id', index),
                "metadata": meta.get('metadata', {})
            }
            
            candidate_sources.append(source_data)
            related_articles.update(article_numbers)
        
        # Sort by adjusted score and take top_k
        candidate_sources.sort(key=lambda x: x['score'], reverse=True)
        final_sources = candidate_sources[:top_k]
        
        # If we found specific articles mentioned, try to include related chunks
        if mentioned_articles and len(final_sources) < top_k:
            for article_num in mentioned_articles:
                if article_num in self.article_to_chunks:
                    for chunk_idx in self.article_to_chunks[article_num]:
                        if chunk_idx < len(self.metadata):
                            meta = self.metadata[chunk_idx]
                            # Check if not already included
                            if not any(s['chunk_id'] == chunk_idx for s in final_sources):
                                source_data = {
                                    "score": 0.9,  # High score for directly mentioned article
                                    "original_score": 0.9,
                                    "content": meta.get('chunk_text', ''),
                                    "document_title": meta.get('document_title', ''),
                                    "document_file": meta.get('source_file', ''),
                                    "category": meta.get('chunk_type', 'general'),
                                    "chunk_type": meta.get('chunk_type', 'general'),
                                    "document_section": meta.get('document_section', ''),
                                    "article_numbers": meta.get('article_numbers', []),
                                    "note_numbers": meta.get('note_numbers', []),
                                    "has_table": meta.get('has_table', False),
                                    "has_list": meta.get('has_list', False),
                                    "document_id": meta.get('source_file', '').replace('.txt', ''),
                                    "chunk_id": chunk_idx,
                                    "metadata": meta.get('metadata', {})
                                }
                                final_sources.append(source_data)
                                if len(final_sources) >= top_k:
                                    break
        
        retrieval_time = time.time() - retrieval_start
        
        print(f"âœ… Retrieved {len(final_sources)} relevant chunks (query_type: {query_type})")
        
        return final_sources, retrieval_time, sorted(list(related_articles))
    
    def prepare_context_for_llm(
        self, 
        sources: List[Dict[str, Any]],
        query_type: str = 'general'
    ) -> str:
        """Prepare context string for LLM with enhanced metadata"""
        
        if not sources:
            return "Ù‡ÛŒÚ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."
        
        context_parts = []
        total_length = 0
        
        for i, source in enumerate(sources, 1):
            content = source['content']
            document = source['document_title']
            section = source['document_section']
            article_nums = source.get('article_numbers', [])
            note_nums = source.get('note_numbers', [])
            chunk_type = source.get('chunk_type', 'general')
            
            # Build metadata line
            metadata_parts = [f"Ù…Ù†Ø¨Ø¹ {i}: {document}"]
            
            if section and section != document:
                metadata_parts.append(f"Ø¨Ø®Ø´: {section}")
            
            if article_nums:
                articles_str = "ØŒ ".join([f"Ù…Ø§Ø¯Ù‡ {num}" for num in article_nums])
                metadata_parts.append(articles_str)
            
            if note_nums:
                notes_str = "ØŒ ".join([f"ØªØ¨ØµØ±Ù‡ {num}" for num in note_nums])
                metadata_parts.append(notes_str)
            
            if source.get('has_table'):
                metadata_parts.append("ğŸ“Š Ø´Ø§Ù…Ù„ Ø¬Ø¯ÙˆÙ„")
            
            metadata_line = " | ".join(metadata_parts)
            source_content = f"[{metadata_line}]\n{content}"
            
            # Check length limit
            if total_length + len(source_content) > self.max_context_length:
                break
            
            context_parts.append(source_content)
            total_length += len(source_content)
        
        context = "\n\n---\n\n".join(context_parts)
        return context
    
    def generate_answer_with_llm(
        self, 
        query: str, 
        context: str,
        query_type: str = 'general',
        related_articles: List[int] = None
    ) -> Tuple[str, str, float]:
        """Generate answer using LLM with query-type-aware prompts"""
        
        generation_start = time.time()
        
        # Query type specific instructions
        type_instructions = {
            'definitional': """
                1. Ø§Ø¨ØªØ¯Ø§ ØªØ¹Ø±ÛŒÙ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø±Ø³Ù…ÛŒ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯
                2. ØªÙˆØ¶ÛŒØ­ Ù…Ø®ØªØµØ± Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø¨Ø¯Ù‡ÛŒØ¯
                3. Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ØŒ Ø´Ø±Ø§ÛŒØ· ÛŒØ§ Ø§Ù„Ø²Ø§Ù…Ø§Øª Ø±Ø§ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯
                4. Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ù…Ø±ØªØ¨Ø· Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ú©Ù†ÛŒØ¯
            """,
            'procedural': """
                1. Ù…Ø±Ø§Ø­Ù„ Ø§Ù†Ø¬Ø§Ù… Ú©Ø§Ø± Ø±Ø§ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯
                2. Ù…Ø¯Ø§Ø±Ú© Ùˆ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯
                3. Ù…Ø±Ø¬Ø¹ Ù…Ø³Ø¦ÙˆÙ„ ÛŒØ§ ÙˆØ§Ø­Ø¯ Ø§Ø¬Ø±Ø§ÛŒÛŒ Ø±Ø§ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯
                4. Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ ÛŒØ§ Ù…Ù‡Ù„Øªâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯ (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)
                5. Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ùˆ Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ Ø±Ø§ Ø¨Ø±Ø¬Ø³ØªÙ‡ Ú©Ù†ÛŒØ¯
            """,
            'eligibility': """
                1. Ø§Ø¨ØªØ¯Ø§ Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… (Ø¨Ù„Ù‡/Ø®ÛŒØ±/Ø¯Ø± Ø´Ø±Ø§ÛŒØ· Ø®Ø§Øµ) Ø¨Ø¯Ù‡ÛŒØ¯
                2. Ø´Ø±Ø§ÛŒØ· Ùˆ Ø§Ù„Ø²Ø§Ù…Ø§Øª Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù„ÛŒØ³Øª Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯
                3. Ø§Ø³ØªØ«Ù†Ø§Ù‡Ø§ Ùˆ Ù…ÙˆØ§Ø±Ø¯ Ø®Ø§Øµ Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯
                4. Ø¨Ù‡ Ù…ÙˆØ§Ø¯ Ùˆ ØªØ¨ØµØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø§Ø±Ø¬Ø§Ø¹ Ø¯Ù‡ÛŒØ¯
            """,
            'numerical': """
                1. Ø§Ø¹Ø¯Ø§Ø¯ØŒ Ø¯Ø±ØµØ¯Ù‡Ø§ ÛŒØ§ Ù…Ø¨Ø§Ù„Øº Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¨Ø±Ø¬Ø³ØªÙ‡ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯
                2. Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ø¬Ø¯ÙˆÙ„ØŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø±Ø§ Ø®Ù„Ø§ØµÙ‡ Ú©Ù†ÛŒØ¯
                3. Ù…Ø­Ø¯ÙˆØ¯Ù‡â€ŒÙ‡Ø§ ÛŒØ§ Ø´Ø±Ø§ÛŒØ· Ù…Ø®ØªÙ„Ù Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯
                4. ÙØ±Ù…ÙˆÙ„ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø±Ø§ Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯
            """,
            'exception': """
                1. Ø§Ø¨ØªØ¯Ø§ Ù‚Ø§Ø¹Ø¯Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ Ø±Ø§ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯
                2. Ø§Ø³ØªØ«Ù†Ø§Ù‡Ø§ Ùˆ Ù…ÙˆØ§Ø±Ø¯ Ø®Ø§Øµ Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯
                3. Ø´Ø±Ø§ÛŒØ· Ø§Ø¹Ù…Ø§Ù„ Ù‡Ø± Ø§Ø³ØªØ«Ù†Ø§ Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯
                4. Ø¨Ù‡ ØªØ¨ØµØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø§Ø±Ø¬Ø§Ø¹ Ø¯Ù‚ÛŒÙ‚ Ø¯Ù‡ÛŒØ¯
            """,
            'timeline': """
                1. Ù…Ù‡Ù„Øªâ€ŒÙ‡Ø§ Ùˆ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ÙˆØ§Ø¶Ø­ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯
                2. Ù†Ù‚Ø§Ø· Ø´Ø±ÙˆØ¹ Ùˆ Ù¾Ø§ÛŒØ§Ù† Ù…Ù‡Ù„Øªâ€ŒÙ‡Ø§ Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯
                3. ØªØ¨Ø¹Ø§Øª Ø¹Ø¯Ù… Ø±Ø¹Ø§ÛŒØª Ù…Ù‡Ù„Øª Ø±Ø§ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯
                4. Ù…ÙˆØ§Ø±Ø¯ ØªÙ…Ø¯ÛŒØ¯ ÛŒØ§ Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯
            """,
            'general': """
                1. Ù¾Ø§Ø³Ø® Ø¬Ø§Ù…Ø¹ Ùˆ Ú©Ø§Ù…Ù„ Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ø¨Ø¯Ù‡ÛŒØ¯
                2. Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ø±Ø§ Ø¨Ø±Ø¬Ø³ØªÙ‡ Ú©Ù†ÛŒØ¯
                3. Ø¨Ù‡ Ù…ÙˆØ§Ø¯ Ùˆ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø§Ø±Ø¬Ø§Ø¹ Ø¯Ù‡ÛŒØ¯
                4. Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²ØŒ Ù…Ø«Ø§Ù„ ÛŒØ§ ØªÙˆØ¶ÛŒØ­ ØªÚ©Ù…ÛŒÙ„ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯
            """
        }
        
        specific_instruction = type_instructions.get(query_type, type_instructions['general'])
        
        # Build article context if available
        article_context = ""
        if related_articles:
            articles_list = "ØŒ ".join([f"Ù…Ø§Ø¯Ù‡ {num}" for num in related_articles])
            article_context = f"\n\nÙ…ÙˆØ§Ø¯ Ù…Ø±ØªØ¨Ø· Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡: {articles_list}"
        
        # Create prompt
        prompt = f"""Ø´Ù…Ø§ ÛŒÚ© Ù…Ø´Ø§ÙˆØ± Ù‚ÙˆØ§Ù†ÛŒÙ† Ùˆ Ù…Ù‚Ø±Ø±Ø§Øª Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ÙØ±Ø¯ÙˆØ³ÛŒ Ù…Ø´Ù‡Ø¯ Ù‡Ø³ØªÛŒØ¯. Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ØŒ Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø± Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø¨Ø¯Ù‡ÛŒØ¯.

Ù†ÙˆØ¹ Ø³Ø¤Ø§Ù„: {query_type}
{article_context}

Ù…Ù‚Ø±Ø±Ø§Øª Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡:
{context}

Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø±: {query}

Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ Ù¾Ø§Ø³Ø®:
{specific_instruction}

Ù†Ú©Ø§Øª Ù…Ù‡Ù…:
- Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ùˆ Ø±Ø³Ù…ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯
- Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ Ø§Ø±Ø¬Ø§Ø¹ Ø¯Ù‚ÛŒÙ‚ Ø¯Ù‡ÛŒØ¯
- Ø¯Ø± ØµÙˆØ±Øª Ø§Ø¨Ù‡Ø§Ù… ÛŒØ§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ø¨ÛŒØ´ØªØ±ØŒ ØµØ±Ø§Ø­ØªØ§Ù‹ Ø§Ø¹Ù„Ø§Ù… Ú©Ù†ÛŒØ¯
- Ø§Ú¯Ø± Ú†Ù†Ø¯ Ø­Ø§Ù„Øª ÛŒØ§ Ø´Ø±Ø§ÛŒØ· Ù…Ø®ØªÙ„Ù ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ù‡Ù…Ù‡ Ø±Ø§ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯
- Ø§Ø² Ø³Ø§Ø®ØªØ§Ø± ÙˆØ§Ø¶Ø­ Ùˆ Ø®ÙˆØ§Ù†Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ø¹Ù†ÙˆØ§Ù†â€ŒÙ‡Ø§ØŒ ÙÙ‡Ø±Ø³ØªØŒ Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ)

Ù¾Ø§Ø³Ø®:"""

        try:
            # Generate response using OpenRouter
            response = self.openai_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://github.com/FUM_RAG",
                    "X-Title": "FUM RAG System",
                },
                model=self.llm_model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.2  # Lower for more factual responses
            )
            
            answer_markdown = response.choices[0].message.content
            
            # Create plain text version
            answer_plain = answer_markdown.replace('**', '').replace('*', '')
            answer_plain = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', answer_plain)
            
            generation_time = time.time() - generation_start
            
            return answer_markdown, answer_plain, generation_time
            
        except Exception as e:
            generation_time = time.time() - generation_start
            error_answer = f"Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {str(e)}\n\nØ¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù‚Ø±Ø±Ø§Øª Ù…ÙˆØ¬ÙˆØ¯:\n{context[:500]}..."
            return error_answer, error_answer, generation_time
    
    def answer_question(self, query: str, top_k: int = 5) -> RAGResponse:
        """Complete RAG pipeline: retrieve context and generate answer"""
        
        total_start = time.time()
        
        print(f"ğŸ” Processing query: {query}")
        
        # Detect query type
        query_type = self.detect_query_type(query)
        print(f"ğŸ¯ Query type detected: {query_type}")
        
        # Retrieve relevant context
        sources, retrieval_time, related_articles = self.retrieve_relevant_context(
            query, top_k, query_type
        )
        
        if not sources:
            total_time = time.time() - total_start
            return RAGResponse(
                query=query,
                answer="Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø³Ø¤Ø§Ù„ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø³Ø¤Ø§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¯ÛŒÚ¯Ø±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.",
                answer_plain_text="Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø³Ø¤Ø§Ù„ ÛŒØ§ÙØª Ù†Ø´Ø¯.",
                sources=[],
                confidence=0.0,
                generation_time=0.0,
                retrieval_time=retrieval_time,
                total_time=total_time,
                context_used="",
                category="Ù†Ø§Ù…Ø´Ø®Øµ",
                query_type=query_type,
                related_articles=related_articles
            )
        
        # Prepare context for LLM
        context = self.prepare_context_for_llm(sources, query_type)
        
        # Generate answer with LLM
        answer_markdown, answer_plain, generation_time = self.generate_answer_with_llm(
            query, context, query_type, related_articles
        )
        
        total_time = time.time() - total_start
        
        # Calculate average confidence
        avg_confidence = (sum(source['score'] for source in sources) / len(sources)) if sources else 0.0
        
        # Get primary category
        primary_category = sources[0]['category'] if sources else "Ù†Ø§Ù…Ø´Ø®Øµ"
        
        # Prepare source information
        source_info = []
        for source in sources:
            source_info.append({
                "source": source['document_title'],
                "document": source['document_title'],
                "category": source['category'],
                "section": source.get('document_section', ''),
                "confidence": source['score'],
                "score": source['score'],
                "document_id": source['document_id'],
                "chunk_type": source.get('chunk_type', 'general'),
                "article_numbers": source.get('article_numbers', []),
                "note_numbers": source.get('note_numbers', []),
                "has_table": source.get('has_table', False),
                "has_list": source.get('has_list', False)
            })
        
        print(f"âœ… Response generated in {total_time:.2f}s (Retrieval: {retrieval_time:.2f}s, Generation: {generation_time:.2f}s)")
        print(f"ğŸ¯ Query type: {query_type}, Related articles: {related_articles}")
        
        return RAGResponse(
            query=query,
            answer=answer_markdown,
            answer_plain_text=answer_plain,
            sources=source_info,
            confidence=avg_confidence,
            generation_time=generation_time,
            retrieval_time=retrieval_time,
            total_time=total_time,
            context_used=context,
            category=primary_category,
            query_type=query_type,
            related_articles=related_articles
        )
    
    def get_system_statistics(self) -> Dict[str, Any]:
        """Get system performance statistics"""
        
        stats = {
            "index_type": "FAISS",
            "total_chunks": len(self.metadata),
            "total_documents": self.faiss_index.ntotal,
            "embedding_model": self.embedding_model_name,
            "embedding_dimension": self.embedding_dim,
            "llm_model": self.llm_model,
            "max_context_length": self.max_context_length,
            "total_articles": len(self.article_to_chunks),
            "indexed_at": self.index_summary.get('indexed_at', 'unknown'),
            "chunking_strategy": self.index_summary.get('chunking_strategy', 'unknown'),
            "status": "active"
        }
        
        return stats


def format_rag_response(response: RAGResponse) -> str:
    """Format RAG response for display"""
    
    formatted = f"""
{'='*80}
ğŸ” Ø³Ø¤Ø§Ù„: {response.query}
ğŸ¯ Ù†ÙˆØ¹ Ø³Ø¤Ø§Ù„: {response.query_type}

ğŸ¤– Ù¾Ø§Ø³Ø®:
{response.answer}

ğŸ“Š Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ:
â”œâ”€â”€ ğŸ·ï¸ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ: {response.category}
â”œâ”€â”€ ğŸ¯ Ø§Ø¹ØªÙ…Ø§Ø¯: {response.confidence:.3f}
â”œâ”€â”€ ğŸ“‹ Ù…ÙˆØ§Ø¯ Ù…Ø±ØªØ¨Ø·: {', '.join([f'Ù…Ø§Ø¯Ù‡ {a}' for a in response.related_articles]) if response.related_articles else 'Ù†Ø¯Ø§Ø±Ø¯'}
â”œâ”€â”€ â±ï¸ Ø²Ù…Ø§Ù† Ú©Ù„: {response.total_time:.2f} Ø«Ø§Ù†ÛŒÙ‡
â”œâ”€â”€ ğŸ” Ø²Ù…Ø§Ù† Ø¬Ø³ØªØ¬Ùˆ: {response.retrieval_time:.2f} Ø«Ø§Ù†ÛŒÙ‡
â””â”€â”€ ğŸ§  Ø²Ù…Ø§Ù† ØªÙˆÙ„ÛŒØ¯: {response.generation_time:.2f} Ø«Ø§Ù†ÛŒÙ‡

ğŸ“š Ù…Ù†Ø§Ø¨Ø¹ ({len(response.sources)} Ù…ÙˆØ±Ø¯):"""

    for i, source in enumerate(response.sources, 1):
        articles = f"Ù…ÙˆØ§Ø¯: {', '.join(map(str, source['article_numbers']))}" if source.get('article_numbers') else ""
        notes = f"ØªØ¨ØµØ±Ù‡â€ŒÙ‡Ø§: {', '.join(map(str, source['note_numbers']))}" if source.get('note_numbers') else ""
        
        formatted += f"""
   {i}. {source['document']} (Ø§Ø¹ØªÙ…Ø§Ø¯: {source['confidence']:.3f})
      Ù†ÙˆØ¹: {source['chunk_type']} | {articles} {notes}"""
    
    formatted += "\n" + "="*80
    
    return formatted


if __name__ == "__main__":
    print("University Rules RAG System V2 - Import this module")
    print("Example: from src.faiss_rag_v2 import UniversityRulesRAG")
