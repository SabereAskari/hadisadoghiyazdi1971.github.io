#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced RAG System with FAISS Integration
Complete RAG pipeline with FAISS vector database and LLM generation
Adapted from Qdrant-based system to use local FAISS index
"""

import os
import json
import time
import numpy as np
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
import re

# Import required packages
try:
    from sentence_transformers import SentenceTransformer
    import faiss
    from openai import OpenAI
    print("âœ… All required packages imported successfully")
except ImportError as e:
    print(f"âŒ Missing required package: {e}")
    print("Please install: pip install sentence-transformers faiss-cpu python-dotenv openai")
    exit(1)

@dataclass
class RAGResponse:
    """Structured response from RAG system"""
    query: str
    answer: str
    answer_plain_text: str
    sources: List[Dict[str, Any]]
    confidence: float
    generation_time: float
    retrieval_time: float
    total_time: float
    context_used: str
    category: str
    cross_references: List[str]
    detected_academic_level: str = 'general'  # New field for academic level
    academic_level_confidence: float = 0.0    # New field for confidence

class UniversityRulesRAGWithLLM:
    def __init__(self, 
                 index_dir: str = "./index",
                 env_path: str = ".env"):
        """Initialize the enhanced RAG system with FAISS"""
        
        # Load environment variables
        load_dotenv(env_path)
        self.openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
        if not self.openrouter_api_key:
            raise ValueError("OPENROUTER_API_KEY environment variable not set or empty. Please set it in .env or environment.")
        self.llm_model = os.getenv("LLM_MODEL", "openai/gpt-4o-mini")
        
        if not self.openrouter_api_key:
            raise ValueError("OPENROUTER_API_KEY environment variable not set")
        
        self.index_dir = Path(index_dir)
        
        # Load index summary to get configuration
        summary_path = self.index_dir / "index_summary.json"
        if not summary_path.exists():
            raise FileNotFoundError(f"Index summary not found: {summary_path}")
        
        with open(summary_path, 'r', encoding='utf-8') as f:
            self.index_summary = json.load(f)
        
        # Get embedding model from summary
        self.embedding_model_name = self.index_summary.get(
            'embedding_model',
            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
        )
        
        # Initialize embedding model
        print(f"ðŸ¤– Loading embedding model: {self.embedding_model_name}")
        self.embedding_model = SentenceTransformer(self.embedding_model_name, device="cpu")
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        
        # Load FAISS index
        index_path = self.index_dir / "faiss_index.bin"
        if not index_path.exists():
            raise FileNotFoundError(f"FAISS index not found: {index_path}")
        
        print(f"ðŸ“‚ Loading FAISS index from: {index_path}")
        self.faiss_index = faiss.read_index(str(index_path))
        print(f"âœ… FAISS index loaded with {self.faiss_index.ntotal} vectors")
        
        # Load metadata
        metadata_path = self.index_dir / "metadata.json"
        if not metadata_path.exists():
            raise FileNotFoundError(f"Metadata not found: {metadata_path}")
        
        print(f"ðŸ“‚ Loading metadata from: {metadata_path}")
        with open(metadata_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        print(f"âœ… Loaded metadata for {len(self.metadata)} chunks")
        
        # Import PDF URLs configuration (optional)
        try:
            from src.pdf_urls_config import get_pdf_url, get_document_title, get_document_category
            self.get_pdf_url = get_pdf_url
            self.get_document_title = get_document_title  
            self.get_document_category = get_document_category
            print("âœ… PDF URLs configuration loaded")
        except ImportError:
            print("âš ï¸ PDF URLs configuration not found, using metadata fallback")
            self.get_pdf_url = lambda doc_id: ""
            self.get_document_title = lambda doc_id: doc_id
            self.get_document_category = lambda doc_id: "Ù†Ø§Ù…Ø´Ø®Øµ"
        
        # Initialize OpenRouter client (OpenAI-compatible)
        print("ðŸ§  Connecting to OpenRouter...")
        self.openai_client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=self.openrouter_api_key,
        )
        print(f"âœ… Connected to OpenRouter with model: {self.llm_model}")
        
        # RAG configuration
        self.max_context_length = 2000  # Max characters for context
        self.max_sources = 5  # Maximum number of sources to retrieve
        self.min_confidence = 0.0  # Minimum confidence threshold
        
        # Academic level mapping for enhanced filtering
        self.academic_levels = {
            'Ú©Ø§Ø±Ø¯Ø§Ù†ÛŒ': 'associate_bachelor',
            'Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ': 'associate_bachelor', 
            'Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ Ø§Ø±Ø´Ø¯': 'masters',
            'Ø§Ø±Ø´Ø¯': 'masters',
            'Ø¯Ú©ØªØ±ÛŒ': 'phd',
            'Ø¯Ú©ØªØ±ÛŒ ØªØ®ØµØµÛŒ': 'phd',
            'Ø¹Ù…ÙˆÙ…ÛŒ': 'general'
        }
        
        # Academic level keywords for query analysis
        self.level_keywords = {
            'associate_bachelor': ['Ú©Ø§Ø±Ø¯Ø§Ù†ÛŒ', 'Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ', 'Ù„ÛŒØ³Ø§Ù†Ø³', 'bachelor', 'Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒ Ø¬Ø¯ÛŒØ¯', 'ØªØ±Ù… Ø§ÙˆÙ„'],
            'masters': ['Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ Ø§Ø±Ø´Ø¯', 'Ø§Ø±Ø´Ø¯', 'ÙÙˆÙ‚ Ù„ÛŒØ³Ø§Ù†Ø³', 'Ù¾Ø§ÛŒØ§Ù†â€ŒÙ†Ø§Ù…Ù‡', 'master', 'Ø§Ø³ØªØ§Ø¯ Ø±Ø§Ù‡Ù†Ù…Ø§'],
            'phd': ['Ø¯Ú©ØªØ±ÛŒ', 'Ø±Ø³Ø§Ù„Ù‡', 'Ø¢Ø²Ù…ÙˆÙ† Ø¬Ø§Ù…Ø¹', 'phd', 'doctorate'],
            'general': ['Ø¹Ù…ÙˆÙ…ÛŒ', 'Ú©Ù„ÛŒ', 'Ù‡Ù…Ù‡', 'ØªÙ…Ø§Ù… Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§']
        }
        
        print("âœ… Enhanced RAG system with FAISS and LLM initialized")
    
    def get_available_categories(self) -> List[str]:
        """Get a list of unique categories from the knowledge base"""
        try:
            from src.pdf_urls_config import get_all_categories
            return get_all_categories()
        except (ImportError, AttributeError):
            # Extract unique categories from metadata
            categories = set()
            for meta in self.metadata:
                if 'category' in meta:
                    categories.add(meta['category'])
                if 'document_type' in meta:
                    categories.add(meta['document_type'])
            return sorted(list(categories)) if categories else ["Ø¹Ù…ÙˆÙ…ÛŒ"]

    def detect_academic_level(self, query: str) -> tuple[str, float]:
        """Detect the academic level from the query with confidence score"""
        query_lower = query.lower()
        
        level_scores = {}
        
        # Score each academic level based on keyword matches
        for level, keywords in self.level_keywords.items():
            score = 0
            for keyword in keywords:
                if keyword in query_lower:
                    # Give higher weight to exact matches
                    if keyword == query_lower.strip():
                        score += 10
                    else:
                        score += len(keyword.split())  # Multi-word keywords get higher scores
            level_scores[level] = score
        
        # Special case: check for combined "Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ Ø§Ø±Ø´Ø¯" pattern
        if 'Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ' in query_lower and 'Ø§Ø±Ø´Ø¯' in query_lower:
            level_scores['masters'] = max(level_scores.get('masters', 0), 15)  # High score for masters
            level_scores['associate_bachelor'] = 0  # Reset bachelor's score
        
        # Find the level with highest score
        if not level_scores or max(level_scores.values()) == 0:
            return 'general', 0.0
        
        best_level = max(level_scores, key=level_scores.get)
        max_score = level_scores[best_level]
        confidence = min(max_score / 15.0, 1.0)  # Normalize to 0-1 range
        
        return best_level, confidence
    
    def retrieve_relevant_context(self, query: str, top_k: int = 5) -> tuple[List[Dict[str, Any]], float, str]:
        """Retrieve relevant context from FAISS index with academic level filtering"""
        
        retrieval_start = time.time()
        
        # Detect academic level first
        detected_level, level_confidence = self.detect_academic_level(query)
        print(f"ðŸŽ¯ Detected academic level: {detected_level} (confidence: {level_confidence:.2f})")
        
        # Embed the query
        query_embedding = self.embedding_model.encode([query])[0].astype('float32')
        
        # Search in FAISS index - get more results for filtering
        k_search = min(top_k * 3, self.faiss_index.ntotal)
        distances, indices = self.faiss_index.search(
            query_embedding.reshape(1, -1), 
            k_search
        )
        
        # Process and filter results by academic level
        filtered_sources = []
        general_sources = []
        other_sources = []
        
        for idx, (distance, index) in enumerate(zip(distances[0], indices[0])):
            if index == -1:  # Invalid index
                continue
            
            # Get metadata for this chunk
            if index >= len(self.metadata):
                continue
                
            meta = self.metadata[index]
            
            # Convert L2 distance to similarity score (inverse)
            # Lower distance = higher similarity
            # Normalize to 0-1 range (approximate)
            similarity_score = 1.0 / (1.0 + distance)
            
            # Extract document info
            source_file = meta.get('source_file', '')
            document_id = source_file.replace('.txt', '') if source_file.endswith('.txt') else source_file
            
            chunk_level = meta.get('academic_level', 'general')
            
            source_data = {
                "score": float(similarity_score),
                "content": meta.get('chunk_text', ''),
                "document_title": self.get_document_title(document_id),
                "document_file": source_file,
                "category_persian": meta.get('document_type', meta.get('category', 'Ø¹Ù…ÙˆÙ…ÛŒ')),
                "chunk_summary": meta.get('chunk_text', '')[:200] + '...',
                "key_topics": meta.get('main_topics', meta.get('keywords', [])),
                "cross_references": meta.get('keywords', []),
                "document_id": document_id,
                "chunk_id": meta.get('chunk_id', index),
                "pdf_url": self.get_pdf_url(document_id),
                "academic_level": chunk_level,
                "article_number": meta.get('article_number', 0),
                "note_number": meta.get('note_number', 0),
                "chunk_type": meta.get('chunk_type', 'unknown'),
                "chunk_index": meta.get('chunk_index', 0)
            }
            
            # Categorize by academic level
            if chunk_level == detected_level:
                filtered_sources.append(source_data)
            elif chunk_level == 'general':
                general_sources.append(source_data)
            else:
                other_sources.append(source_data)
        
        # Combine results: prioritize detected level, then general, then others
        final_sources = filtered_sources[:top_k]
        
        # If we don't have enough results from the detected level, add general sources
        if len(final_sources) < top_k:
            needed = top_k - len(final_sources)
            final_sources.extend(general_sources[:needed])
        
        # If still not enough, add from other levels
        if len(final_sources) < top_k:
            needed = top_k - len(final_sources)
            final_sources.extend(other_sources[:needed])
        
        retrieval_time = time.time() - retrieval_start
        
        print(f"âœ… Retrieved {len(final_sources)} relevant chunks")
        
        return final_sources, retrieval_time, detected_level
    
    def prepare_context_for_llm(self, sources: List[Dict[str, Any]], detected_level: str = 'general') -> tuple[str, List[str]]:
        """Prepare context string for LLM with source tracking and academic level information"""
        
        if not sources:
            return "Ù‡ÛŒÚ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.", []
        
        context_parts = []
        cross_references = set()
        total_length = 0
        
        for i, source in enumerate(sources, 1):
            content = source['content']
            category = source['category_persian']
            document = source['document_title']
            document_id = source['document_id']
            pdf_url = source['pdf_url']
            academic_level = source.get('academic_level', 'general')
            article_number = source.get('article_number', 0)
            note_number = source.get('note_number', 0)
            
            # Enhanced source metadata with academic level and article information
            level_info = f" - Ù…Ù‚Ø·Ø¹: {category}" if category else ""
            article_info = f" - Ù…Ø§Ø¯Ù‡ {article_number}" if article_number > 0 else ""
            note_info = f" - ØªØ¨ØµØ±Ù‡ {note_number}" if note_number > 0 else ""
            
            # Create proper hyperlink if PDF URL is available
            if pdf_url and pdf_url != "PDF_URL":
                pdf_link_text = f" - [Ù…Ø´Ø§Ù‡Ø¯Ù‡ PDF]({pdf_url})"
            else:
                pdf_link_text = " - [PDF Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª]"
            
            source_info = f"[Ù…Ù†Ø¨Ø¹ {i}: {document}{level_info}{article_info}{note_info}{pdf_link_text}]"
            source_content = f"{source_info}\n{content}"
            
            # Check length limit
            if total_length + len(source_content) > self.max_context_length:
                break
            
            context_parts.append(source_content)
            total_length += len(source_content)
            
            # Collect cross-references
            cross_references.update(source.get('cross_references', []))
        
        context = "\n\n".join(context_parts)
        return context, list(cross_references)
    
    def generate_answer_with_llm(self, query: str, context: str, cross_references: List[str], detected_level: str = 'general') -> tuple[str, str, float]:
        """Generate answer using LLM via OpenRouter with academic level awareness"""
        
        generation_start = time.time()
        
        # Prepare cross-reference information
        cross_ref_info = ""
        if cross_references:
            cross_ref_info = f"\n\nÙ…Ù‚Ø±Ø±Ø§Øª Ù…Ø±ØªØ¨Ø·: {', '.join(cross_references)}"
        
        # Academic level specific context and instructions
        level_context = {
            'associate_bachelor': 'Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ú©Ø§Ø±Ø¯Ø§Ù†ÛŒ Ùˆ Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ',
            'masters': 'Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ Ø§Ø±Ø´Ø¯',
            'phd': 'Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø¯Ú©ØªØ±ÛŒ ØªØ®ØµØµÛŒ',
            'general': 'ØªÙ…Ø§Ù… Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù†'
        }
        
        level_specific_info = {
            'associate_bachelor': 'ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ø§ÛŒÙ† Ù¾Ø§Ø³Ø® Ù…Ø®ØµÙˆØµ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ú©Ø§Ø±Ø¯Ø§Ù†ÛŒ Ùˆ Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ Ø§Ø³Øª. Ù…ÙˆØ§Ø±Ø¯ÛŒ Ù…Ø§Ù†Ù†Ø¯ Ù¾Ø§ÛŒØ§Ù†â€ŒÙ†Ø§Ù…Ù‡ØŒ Ø±Ø³Ø§Ù„Ù‡ØŒ ÛŒØ§ Ø¢Ø²Ù…ÙˆÙ† Ø¬Ø§Ù…Ø¹ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù…Ù‚Ø§Ø·Ø¹ Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø³Øª.',
            'masters': 'ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ø§ÛŒÙ† Ù¾Ø§Ø³Ø® Ù…Ø®ØµÙˆØµ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ Ø§Ø±Ø´Ø¯ Ø§Ø³Øª. Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø§ÛŒÙ† Ù…Ù‚Ø·Ø¹ Ø¨Ø§ÛŒØ¯ Ù¾Ø§ÛŒØ§Ù†â€ŒÙ†Ø§Ù…Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ù†Ø¯ Ùˆ Ø§Ø³ØªØ§Ø¯ Ø±Ø§Ù‡Ù†Ù…Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†Ù†Ø¯.',
            'phd': 'ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ø§ÛŒÙ† Ù¾Ø§Ø³Ø® Ù…Ø®ØµÙˆØµ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø¯Ú©ØªØ±ÛŒ ØªØ®ØµØµÛŒ Ø§Ø³Øª. Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø§ÛŒÙ† Ù…Ù‚Ø·Ø¹ Ø¨Ø§ÛŒØ¯ Ø±Ø³Ø§Ù„Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ù†Ø¯ Ùˆ Ø¢Ø²Ù…ÙˆÙ† Ø¬Ø§Ù…Ø¹ Ø¨Ú¯Ø°Ø±Ø§Ù†Ù†Ø¯.',
            'general': 'Ø§ÛŒÙ† Ù¾Ø§Ø³Ø® Ø´Ø§Ù…Ù„ Ù…Ù‚Ø±Ø±Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø§Ø³Øª.'
        }
        
        target_audience = level_context.get(detected_level, 'ØªÙ…Ø§Ù… Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù†')
        level_instruction = level_specific_info.get(detected_level, '')
        
        # Create enhanced academic-level-aware prompt
        prompt = f"""Ø´Ù…Ø§ ÛŒÚ© Ù…Ø´Ø§ÙˆØ± Ù‚ÙˆØ§Ù†ÛŒÙ† Ùˆ Ù…Ù‚Ø±Ø±Ø§Øª Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ÙØ±Ø¯ÙˆØ³ÛŒ Ù…Ø´Ù‡Ø¯ Ù‡Ø³ØªÛŒØ¯. Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ØŒ Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø± Ù¾Ø§Ø³Ø® Ú©Ø§Ù…Ù„ÛŒ Ø¨Ø¯Ù‡ÛŒØ¯.

Ù…Ø®Ø§Ø·Ø¨ Ù‡Ø¯Ù: {target_audience}
{level_instruction}

Ù…Ù‚Ø±Ø±Ø§Øª Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡:
{context}
{cross_ref_info}

Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø±: {query}

Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ Ù¾Ø§Ø³Ø®:
1. Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ùˆ Ù…Ø®ØµÙˆØµ {target_audience} Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯
2. Ø§Ú¯Ø± Ø³Ø¤Ø§Ù„ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù…Ù‚Ø·Ø¹ ØªØ­ØµÛŒÙ„ÛŒ Ø¯ÛŒÚ¯Ø±ÛŒ Ø§Ø³ØªØŒ ØµØ±Ø§Ø­Øª Ø§Ø¹Ù„Ø§Ù… Ú©Ù†ÛŒØ¯
3. Ù¾Ø§Ø³Ø® Ø¨Ø§ÛŒØ¯ Ø¹Ù…Ù„ÛŒØŒ Ù…ÙÛŒØ¯ Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ø¨Ø§Ø´Ø¯
4. Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯
5. Ø¯Ø± Ù¾Ø§Ø³Ø®ØŒ Ø¨Ù‡ Ø¬Ø§ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ PDFØŒ Ø§Ø² Ø¹Ø¨Ø§Ø±Ø§Øª Ú©ÙˆØªØ§Ù‡ Ù…Ø§Ù†Ù†Ø¯ "[Ù…Ø´Ø§Ù‡Ø¯Ù‡ PDF]" ÛŒØ§ "([Ù…Ù†Ø¨Ø¹])" Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
6. Ø§Ú¯Ø± Ú†Ù†Ø¯ÛŒÙ† Ø¨Ø®Ø´ Ù…Ø±ØªØ¨Ø· ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ù‡Ù…Ù‡ Ø±Ø§ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯
7. Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ù†Ú©Ø§Øª Ù…Ù‡Ù… ÛŒØ§ Ø§Ø³ØªØ«Ù†Ø§Ù‡Ø§ØŒ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ø±Ø¬Ø³ØªÙ‡ Ú©Ù†ÛŒØ¯
8. Ø¯Ø± Ù…ØªÙ† Ù¾Ø§Ø³Ø®ØŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø³Ø§Ø¯Ù‡ Ù…Ø§Ù†Ù†Ø¯ ([Ù…Ù†Ø¨Ø¹ X]) Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯

Ù¾Ø§Ø³Ø®:"""

        try:
            # Generate response using OpenRouter
            response = self.openai_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://github.com/FUM_RAG",
                    "X-Title": "FUM RAG System",
                },
                model=self.llm_model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3  # Slightly higher for more natural responses
            )
            
            answer_markdown = response.choices[0].message.content
            
            # Create a plain text version for copy-pasting
            answer_plain = answer_markdown.replace('**', '').replace('*', '')
            answer_plain = answer_plain.replace('[Ù…Ø´Ø§Ù‡Ø¯Ù‡ PDF]', '').replace('([Ù…Ù†Ø¨Ø¹])', '')
            
            generation_time = time.time() - generation_start
            
            return answer_markdown, answer_plain, generation_time
            
        except Exception as e:
            generation_time = time.time() - generation_start
            error_answer = f"Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {str(e)}\n\nØ¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù‚Ø±Ø±Ø§Øª Ù…ÙˆØ¬ÙˆØ¯:\n{context[:500]}..."
            return error_answer, error_answer, generation_time
    
    def answer_question(self, query: str, top_k: int = 5) -> RAGResponse:
        """Complete RAG pipeline: retrieve context and generate answer with academic level awareness"""
        
        total_start = time.time()
        
        print(f"ðŸ” Processing query: {query}")
        
        # Step 1: Retrieve relevant context with academic level filtering
        sources, retrieval_time, detected_level = self.retrieve_relevant_context(query, top_k)
        
        # Get academic level confidence for response
        detected_level_with_confidence, level_confidence = self.detect_academic_level(query)
        
        if not sources:
            total_time = time.time() - total_start
            return RAGResponse(
                query=query,
                answer="Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø³Ø¤Ø§Ù„ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø³Ø¤Ø§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¯ÛŒÚ¯Ø±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.",
                answer_plain_text="Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø³Ø¤Ø§Ù„ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø³Ø¤Ø§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¯ÛŒÚ¯Ø±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.",
                sources=[],
                confidence=0.0,
                generation_time=0.0,
                retrieval_time=retrieval_time,
                total_time=total_time,
                context_used="",
                category="Ù†Ø§Ù…Ø´Ø®Øµ",
                cross_references=[],
                detected_academic_level=detected_level,
                academic_level_confidence=level_confidence
            )
        
        # Step 2: Prepare context for LLM with academic level information
        context, cross_references = self.prepare_context_for_llm(sources, detected_level)
        
        # Step 3: Generate answer with LLM and academic level awareness
        answer_markdown, answer_plain, generation_time = self.generate_answer_with_llm(
            query,
            context,
            cross_references,
            detected_level
        )
        
        # Linkify inline references like ([Ù…Ù†Ø¨Ø¹ 1]) using the retrieved sources
        answer_markdown = self._linkify_references(answer_markdown, sources)
        
        total_time = time.time() - total_start
        
        # Calculate average confidence from sources
        avg_confidence = (sum(source['score'] for source in sources) / len(sources)) if sources else 0.0
        
        # Get primary category from best source
        primary_category = sources[0]['category_persian'] if sources else "Ù†Ø§Ù…Ø´Ø®Øµ"
        
        # Prepare source information for response
        source_info = []
        for source in sources:
            source_info.append({
                "source": source['document_title'],  # For compatibility
                "document": source['document_title'],
                "category": source['category_persian'], 
                "confidence": source['score'],
                "score": source['score'],
                "topics": source['key_topics'],
                "document_id": source['document_id'],
                "pdf_url": source['pdf_url'],
                "academic_level": source.get('academic_level', 'general'),
                "article_number": source.get('article_number', 0),
                "note_number": source.get('note_number', 0)
            })
        
        print(f"âœ… Response generated in {total_time:.2f}s (Retrieval: {retrieval_time:.2f}s, Generation: {generation_time:.2f}s)")
        print(f"ðŸŽ¯ Academic level: {detected_level}")
        
        return RAGResponse(
            query=query,
            answer=answer_markdown,
            answer_plain_text=answer_plain,
            sources=source_info,
            confidence=avg_confidence,
            generation_time=generation_time,
            retrieval_time=retrieval_time,
            total_time=total_time,
            context_used=context,
            category=primary_category,
            cross_references=cross_references,
            detected_academic_level=detected_level,
            academic_level_confidence=level_confidence
        )
    
    def batch_answer_questions(self, queries: List[str]) -> List[RAGResponse]:
        """Answer multiple questions in batch"""
        
        print(f"ðŸ”„ Processing {len(queries)} queries in batch...")
        responses = []
        
        for i, query in enumerate(queries, 1):
            print(f"\nðŸ“ Query {i}/{len(queries)}: {query[:50]}...")
            response = self.answer_question(query)
            responses.append(response)
            
            # Small delay to avoid rate limiting
            time.sleep(0.5)
        
        print(f"âœ… Batch processing complete")
        return responses
    
    def get_system_statistics(self) -> Dict[str, Any]:
        """Get system performance statistics"""
        
        stats = {
            "index_type": "FAISS",
            "total_documents": self.faiss_index.ntotal,
            "total_chunks": len(self.metadata),
            "embedding_model": self.embedding_model_name,
            "embedding_dimension": self.embedding_dim,
            "llm_model": self.llm_model,
            "max_context_length": self.max_context_length,
            "min_confidence_threshold": self.min_confidence,
            "indexed_at": self.index_summary.get('indexed_at', 'unknown'),
            "status": "active"
        }
        
        return stats
    
    def _linkify_references(self, answer_markdown: str, sources: List[Dict[str, Any]]) -> str:
        """Turn inline reference markers like ([Ù…Ù†Ø¨Ø¹ 1]) or [Ù…Ù†Ø¨Ø¹ 1] into markdown links using sources order."""
        if not answer_markdown or not sources:
            return answer_markdown
        
        # Build index -> url map (1-based)
        index_to_url = {}
        for i, s in enumerate(sources, start=1):
            url = s.get('pdf_url') or ''
            if url and isinstance(url, str) and url.startswith(('http://', 'https://')):
                index_to_url[i] = url
        
        if not index_to_url:
            return answer_markdown
        
        # Replace ([Ù…Ù†Ø¨Ø¹ N]) first
        def repl_paren(m):
            idx = int(m.group(1))
            url = index_to_url.get(idx)
            return f"[Ù…Ù†Ø¨Ø¹ {idx}]({url})" if url else m.group(0)
        
        result = re.sub(r"\(\[Ù…Ù†Ø¨Ø¹\s+(\d+)\]\)", repl_paren, answer_markdown)
        
        # Replace bare [Ù…Ù†Ø¨Ø¹ N] not already linked (no immediate '(')
        def repl_bare(m):
            idx = int(m.group(1))
            url = index_to_url.get(idx)
            return f"[Ù…Ù†Ø¨Ø¹ {idx}]({url})" if url else m.group(0)
        
        result = re.sub(r"\[Ù…Ù†Ø¨Ø¹\s+(\d+)\](?!\()", repl_bare, result)
        
        return result

def format_rag_response(response: RAGResponse) -> str:
    """Format RAG response for display"""
    
    formatted = f"""
{'='*80}
ðŸ” Ø³Ø¤Ø§Ù„: {response.query}

ðŸ¤– Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ:
{response.answer}

ðŸ“Š Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ:
â”œâ”€â”€ ðŸ·ï¸ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ: {response.category}
â”œâ”€â”€ ðŸŽ¯ Ø§Ø¹ØªÙ…Ø§Ø¯: {response.confidence:.3f}
â”œâ”€â”€ ðŸŽ“ Ù…Ù‚Ø·Ø¹ ØªØ­ØµÛŒÙ„ÛŒ: {response.detected_academic_level} ({response.academic_level_confidence:.2f})
â”œâ”€â”€ â±ï¸ Ø²Ù…Ø§Ù† Ú©Ù„: {response.total_time:.2f} Ø«Ø§Ù†ÛŒÙ‡
â”œâ”€â”€ ðŸ” Ø²Ù…Ø§Ù† Ø¬Ø³ØªØ¬Ùˆ: {response.retrieval_time:.2f} Ø«Ø§Ù†ÛŒÙ‡
â””â”€â”€ ðŸ§  Ø²Ù…Ø§Ù† ØªÙˆÙ„ÛŒØ¯: {response.generation_time:.2f} Ø«Ø§Ù†ÛŒÙ‡

ðŸ“š Ù…Ù†Ø§Ø¨Ø¹ ({len(response.sources)} Ù…ÙˆØ±Ø¯):"""

    for i, source in enumerate(response.sources, 1):
        pdf_link = ""
        if source.get('pdf_url'):
            pdf_link = f" ðŸ“„ [Ù…Ø´Ø§Ù‡Ø¯Ù‡ PDF]({source['pdf_url']})"
        
        formatted += f"""
   {i}. {source['document']} (Ø§Ø¹ØªÙ…Ø§Ø¯: {source['confidence']:.3f}){pdf_link}
      Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ: {source['category']}
      Ù…Ù‚Ø·Ø¹: {source.get('academic_level', 'general')}"""
    
    if response.cross_references:
        formatted += f"""

ðŸ”— Ù…Ù‚Ø±Ø±Ø§Øª Ù…Ø±ØªØ¨Ø·: {', '.join(response.cross_references)}"""
    
    formatted += "\n" + "="*80
    
    return formatted

# Usage and testing functions
def test_faiss_rag_system():
    """Test the FAISS-based RAG system"""
    
    print("ðŸ§ª Testing FAISS-based RAG System")
    print("="*80)
    
    # Initialize system
    rag = UniversityRulesRAGWithLLM()
    
    # Test queries
    test_queries = [
        "Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ù…Ø³Ø¦ÙˆÙ„ Ú©ÛŒØ³Øª Ùˆ Ú†Ù‡ ÙˆØ¸Ø§ÛŒÙÛŒ Ø¯Ø§Ø±Ø¯ØŸ",
        "Ú†Ú¯ÙˆÙ†Ù‡ Ù†Ø´Ø§Ù†ÛŒ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ÙØ±Ø¯ÙˆØ³ÛŒ Ù…Ø´Ù‡Ø¯ Ø±Ø§ Ø¯Ø± Ù…Ù‚Ø§Ù„Ø§Øª Ø¯Ø±Ø¬ Ú©Ù†Ù…ØŸ",
        "Ù‚ÙˆØ§Ù†ÛŒÙ† Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ø² Ù¾Ø§ÛŒØ§Ù†â€ŒÙ†Ø§Ù…Ù‡ Ú†ÛŒØ³ØªØŸ"
    ]
    
    # Process queries
    for query in test_queries:
        print(f"\nðŸ” Query: {query}")
        response = rag.answer_question(query)
        print(format_rag_response(response))
        print("\n")
    
    # System statistics
    stats = rag.get_system_statistics()
    print("ðŸ“Š System Statistics:")
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    return rag

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--test":
        test_faiss_rag_system()
    else:
        print("FAISS RAG System - Import this module or use --test flag")
        print("Example: from src.faiss_rag_with_llm import UniversityRulesRAGWithLLM")
