---
layout: persian  # یا single با کلاس rtl-layout
classes: wide rtl-layout
dir: rtl
title: "BERT"
permalink: /teaching/studenteffort/patterneffort/BERT/
author_profile: true

header:
  overlay_image: "/assets/images/background.jpg"
  overlay_filter: 0.3
  overlay_color: "#5e616c"
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"

---

# Bert

## مدل‌های زبانی

چند سال پیش مربی یک کلاس برنامه و تفکر الگوریتمی بودم. در بین پروژه‌هایی که دانش‌آموزان انجام می‌دادن یک روز یکی از دانش‌آموزان کلاس ایده ساختن محصولی مشابه [کورتانا](https://en.wikipedia.org/wiki/Cortana_\(virtual_assistant\)) ویندوز پیش اومد.

این دانش‌آموز می‌خواست به کمک مجموعه بزرگی از شرط‌ها شرایط رو برای یک گفت و گو اولیه فراهم کنه.  
اگر گفت:«سلام.» برنامه بگه :«سلام چطوری؟» و … . خوب این کار برای یک دانش‌آموز ممکن نبود. حتی برای یک گروه عظیم از کارمندان یک شرکت هم حتی شاید ممکن نباشه. پس اینجا به یک برنامه‌ای نیاز داریم که جملات و گفت و گوها رو بیشتر از یک string خشک و ساده ببینه.

### تبدیل کردن به قطعات کوچک تر از جمله و عبارت

برای این که کامپیوتر بتونه برای خودش دیکشنری از معانی و مفاهیم جملات درست کنه درست کردن دیکشنری از کل عبارت‌های ممکن کار سختی هست. در طرف دیگر اگر زبان را از دید بخش‌های کوچک تر از کلمه یعنی حرف ببینیم کامپیوتر به سختی درکی از زبان بدست می‌آورد.

<h4>
<a href="https://en.wikipedia.org/wiki/Large_language_model#Tokenization" style="text-decoration:underline; color:green;" target="_blank">
Tokenization
</a>
</h4>

گاهی ممکن است نیاز باشد که درباره کلماتی جدید صحبت به میان بیاد پس به حروف کوچک و تعیین کردن شرط و شروط در مورد حروف داریم ولی اگر فقط حروف باشد پیچیده می‌شود پس کلمات هم باید باشند. از طرفی پسوندهای معنی دار هم جایگاه مهمی در زبان دارند و تعیین کردن شرط برای آن‌ها هم می‌تواند کمک کننده باشد پس با معرفی کردن همه آن‌ها به ماشین شاید به حل مسئله چت بات برسیم. به فرایند تبدیل کردن جملات و عبارت‌های متنی به این بخش‌های قابل فهم برای کامپیوتر Tokenization می‌گویند.  
روش‌های متفاوتی برای این کار وجود دارد. در مقاله BERT از روش **WordPiece Tokenization** استفاده شده است.

در این روش، برخلاف روش‌های ساده‌تری مثل جدا کردن جمله بر اساس فاصله‌ها (space) یا استفاده از فهرستی ثابت از کلمات، مدل ابتدا یک واژگان (vocabulary) پایه دارد و سپس سعی می‌کند کلمات جدید را به ترکیبی از قطعات کوچک‌تر که در آن واژگان موجود هستند تبدیل کند.

برای مثال اگر واژه‌ی «playing» در واژگان نباشد، مدل می‌تواند آن را به صورت «play» و «##ing» بنویسد. علامت «##» نشان می‌دهد که بخش دوم در ادامه‌ی بخش قبلی آمده است، نه در ابتدای یک کلمه‌ی جدید.

این ایده از روش‌های مشابهی مانند **Byte Pair Encoding (BPE)** الهام گرفته است. در روش BPE، پرتکرارترین جفت نویسه‌ها به تدریج ترکیب می‌شوند تا یک واژگان فشرده و کارآمد ساخته شود. WordPiece نیز فرآیندی شبیه دارد، با این تفاوت که به جای تکرار ساده‌ی نویسه‌ها، احتمال وقوع توالی‌ها را در کل داده‌های آموزشی بررسی می‌کند و بر اساس آن تصمیم می‌گیرد چه ترکیب‌هایی باید در واژگان بمانند.

هدف از این روش ایجاد تعادلی بین فهم دقیق واژه‌ها و توانایی برخورد با کلمات ناشناخته است. به این ترتیب مدل می‌تواند هم واژه‌های پرتکرار را به عنوان یک واحد مستقل یاد بگیرد، و هم واژه‌های جدید را از ترکیب قطعات شناخته‌شده بسازد.

به کمک این نوع Tokenization، مدل BERT قادر است با متون بسیار متنوعی روبه‌رو شود و در عین حال معنا و ساختار واژه‌ها را تا حد زیادی حفظ کند. این مرحله یکی از کلیدهای اصلی موفقیت BERT در درک زبان طبیعی به شمار می‌رود.