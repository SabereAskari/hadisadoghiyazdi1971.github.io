ุฎู ุนุงู ๐
ุงูุฌุง ฺฉ ูุงู ฺฉุงูู ู ุขููุฒุด ุจู ูุงู `README.md` (ุง ุงฺฏุฑ ุฎูุงุณุชุฏ ุฏููุงู ุทุจู ุฎูุงุณุชูโุชุงู `ReadMe.rd`) ุจุฑุง ูพุฑูฺูโ **Scrapy + ุงูุฏฺฉุณโุณุงุฒ** ุดูุง ุขูุงุฏู ฺฉุฑุฏูโุงู.
ุงู ูุงู ุจุฑุง ุฒูุงู ุทุฑุงุญ ุดุฏู ฺฉู ูุฏุช ุจุนุฏ ุจู ูพุฑูฺู ุจุฑฺฏุฑุฏุฏ ู ุฏูู ุงุฏุชุงู ุจุงุฏ ุจุงุฏ ฺู ฺฉูุฏ ู ฺุฑุง ูุฑ ูุฑุญูู ููู ุงุณุช.

---

## ๐ **ูุงู: ReadMe.rd**

````markdown
# ๐ท๏ธ ูพุฑูฺู ุฎุฒุฏู ู ุงูุฏฺฉุณโุณุงุฒ ูุญุชูุง ุณุงุชโูุง ุจุง Scrapy ู FAISS

ุงู ูุงู ุจุฑุง ูุฑูุฑ ฺฉุงูู ูุฑุงุญู ฺฉุงุฑ ุทุฑุงุญ ุดุฏู ุชุง ุจุนุฏ ุงุฒ ฺฏุฐุดุช ุฒูุงูุ ูุฑุงููุด ุง ุณุฑุฏุฑฺฏู ูพุด ูุงุฏ.  
ูพุฑูฺู ุงุฒ ุฏู ุจุฎุด ุงุตู ุชุดฺฉู ุดุฏู ุงุณุช:

1. **ุฎุฒุฏู (Crawling)** ุฏุงุฏูโูุง ูุจ ุจุง `Scrapy`  
2. **ุงูุฏฺฉุณโุณุงุฒ (Indexing)** ุฏุงุฏูโูุง ุจุง ุงุณฺฉุฑูพุช `AdvancedFAISSIndexCreator`

---

## ๐ ุจุฎุด ฑ โ ุงุฌุฑุง ุฎุฒูุฏู Scrapy

### ๐งฉ ูุฏู
ุฌูุนโุขูุฑ ูุชูโูุง ูุงุฑุณ ุงุฒ ุณุงุชโูุง ุฏูุฎูุงู (ูุซู `partlasticgroup.com`)  
ู ุฐุฎุฑูโ ุขูโูุง ุฏุฑ ูพูุดูโ `documents/` ุจุฑุง ุขูุงุฏูโุณุงุฒ ุงูุฏฺฉุณ.

---

### โ๏ธ 1. ูุนุงูโุณุงุฒ ูุญุท ูุฌุงุฒ (Virtual Env)
ุงุจุชุฏุง ูุญุท ูพุงุชูู ูุฎุตูุต ูพุฑูฺู ุฑุง ูุนุงู ฺฉูุฏ:

```bash
H:\path\to\rag_env\Scripts\activate
````

ุงฺฏุฑ ูููุฒ Scrapy ูุตุจ ูุดุฏู:

```bash
pip install scrapy
```

ุจุฑุง ุงุทููุงู ุงุฒ ูุตุจ ุตุญุญ:

```bash
scrapy version
```

---

### ๐๏ธ 2. ุณุงุฎุช ูพุฑูฺู Scrapy (ููุท ุจุงุฑ ุงูู)

ุงฺฏุฑ ูููุฒ ูพุฑูฺู Scrapy ูุฏุงุฑุฏ:

```bash
scrapy startproject partlastic_crawler
cd partlastic_crawler
```

ูพูุดูโุง ุณุงุฎุชู ูโุดูุฏ ุดุงูู:

```
partlastic_crawler/
  โโโ scrapy.cfg
  โโโ partlastic_crawler/
      โโโ spiders/
      โโโ settings.py
```

---

### ๐ธ๏ธ 3. ุณุงุฎุช ุง ูุฑุงุด ูุงู ุฎุฒูุฏู (spider)

ุฏุฑ ูุณุฑ:

```
partlastic_crawler/spiders/partlastic_spider.py
```

ฺฉุฏ ุฒุฑ ุฑุง ูุฑุงุฑ ุฏูุฏ ุง ูุฑุงุด ฺฉูุฏ:

```python
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
import re, os

class PartlasticSpider(CrawlSpider):
    name = "partlastic"
    allowed_domains = ["partlasticgroup.com"]
    start_urls = [
        "https://partlasticgroup.com/",
        # ุฏุฑ ุตูุฑุช ูุงุฒ ุขุฏุฑุณโูุง ุฌุฏุฏ ุฑุง ุฏุฑ ุงู ูุณุช ุงุถุงูู ฺฉูุฏ
    ]

    rules = (
        Rule(LinkExtractor(allow=()), callback="parse_page", follow=True),
    )

    def clean_text(self, html):
        text = re.sub(r"<[^>]+>", " ", html)
        text = re.sub(r"\s+", " ", text)
        text = re.sub(r"[\r\n\t]+", " ", text)
        return text.strip()

    def parse_page(self, response):
        text = self.clean_text(response.text)
        if len(text) > 300 and re.search(r"[\u0600-\u06FF]", text):  # ููุท ุตูุญุงุช ูุงุฑุณ
            os.makedirs("documents", exist_ok=True)
            filename = re.sub(r"[^a-zA-Z0-9]+", "_", response.url)[:80] + ".txt"
            path = os.path.join("documents", filename)
            if os.path.exists(path):  # ุงุฒ ุชฺฉุฑุงุฑ ุฌููฺฏุฑ ูโฺฉูุฏ
                self.log(f"โฉ Skipped existing: {path}")
                return
            with open(path, "w", encoding="utf-8") as f:
                f.write(text)
            self.log(f"โ Saved clean text: {path}")
```

---

### ๐ 4. ุงุฌุฑุง ุฎุฒูุฏู

ุงุฒ ูุณุฑ ุงุตู ูพุฑูฺู ฺฉู `scrapy.cfg` ุฏุฑ ุขู ุงุณุชุ ุงุฌุฑุง ฺฉูุฏ:

```bash
scrapy crawl partlastic
```

ุงฺฏุฑ ุฎุทุง ุฏุงุฏ ฺฉู Scrapy ุดูุงุฎุชู ูุดุฏ:

```bash
python -m scrapy crawl partlastic
```

---

### ๐ 5. ูุชุงุฌ ุฎุฒุฏู

ูพุณ ุงุฒ ูพุงุงู ฺฉุงุฑุ ูพูุดูโุง ุณุงุฎุชู ูโุดูุฏ:

```
documents/
   โโโ partlasticgroup_com_index.txt
   โโโ partlasticgroup_com_about_us.txt
   โโโ ...
```

ูุฑ ูุงู ุญุงู ูุชู ุชูุฒ ู ุจุฏูู HTML ุตูุญุงุช ุณุงุช ุงุณุช.

---

### ๐ก ูฺฉุงุช ููู ุจุฑุง ุฏูุนุงุช ุจุนุฏ:

| ูููุนุช               | ุชูุถุญ                                                                   |
| -------------------- | ----------------------------------------------------------------------- |
| ุงูุฒูุฏู ุขุฏุฑุณโูุง ุฌุฏุฏ | ฺฉุงู ุงุณุช ุฏุฑ `start_urls` ุจููุณุฏ                                        |
| ุญุฐู ุขุฏุฑุณโูุง ูุฏู   | ุขุฏุฑุณโูุง ูุฏู ุฑุง ุงุฒ `start_urls` ูพุงฺฉ ฺฉูุฏ (ุงุทูุงุนุงุช ูุจู ูุญููุธ ูโูุงูุฏ) |
| ุงุฌุฑุง ูุฌุฏุฏ ุฎุฒูุฏู     | ูุงูโูุง ุฌุฏุฏ ุจู `documents` ุงุถุงูู ูโุดููุฏ                              |
| ุฌููฺฏุฑ ุงุฒ ุจุงุฒููุณ  | ุดุฑุท `if os.path.exists(path)` ูุงูุน ูโุดูุฏ                               |
| ุดุฑูุน ุงุฒ ุตูุฑ          | ูพูุดู `documents/` ุฑุง ุฏุณุช ูพุงฺฉ ฺฉูุฏ ุชุง ุฏูุจุงุฑู ุณุงุฎุชู ุดูุฏ                  |

---

## ๐ ุจุฎุด ฒ โ ุงูุฏฺฉุณโุณุงุฒ ุจุง FAISS

ุจุนุฏ ุงุฒ ุงูฺฉู ุฏุงุฏูโูุง ุฏุฑ ูพูุดูโ `documents` ุขูุงุฏู ุดุฏูุฏุ ูุฑุญููู ุฏูู ุขุบุงุฒ ูโุดูุฏ.

---

### โ๏ธ 1. ูพุดโูุงุฒูุง

ุฏุฑ ููุงู ูุญุท ูุฌุงุฒ (`rag_env`):

```bash
pip install sentence-transformers faiss-cpu scikit-learn tqdm numpy
```

---

### โ๏ธ 2. ุงุฌุฑุง ุงูุฏฺฉุณโุณุงุฒ

ูุงู ุงุตู ุงูุฏฺฉุณโุณุงุฒ ุดูุง:

```
83092fb9-9cb3-4d86-93a7-5d26f33fa1b7.py
```

ุฑุง ุฏุฑ ูุณุฑ ูพุฑูฺู ุงุฌุฑุง ฺฉูุฏ:

```bash
python 83092fb9-9cb3-4d86-93a7-5d26f33fa1b7.py
```

---

### ๐ฆ 3. ุฎุฑูุฌโูุง ููุง

ูพุณ ุงุฒ ุงุฌุฑุงุ ุฏุฑ ูพูุดูโ `index_v4_upgraded/` ูุงูโูุง ุฒุฑ ุณุงุฎุชู ูโุดูุฏ:

```
index_v4_upgraded/
  โโโ faiss_index_upgraded.index
  โโโ enhanced_metadata.json
  โโโ comprehensive_summary.json
```

ุงูโูุง ุจุฑุง ุฌุณุชุฌู ู ูพุงุณุฎโุฏู ููุดููุฏ (RAG) ุงุณุชูุงุฏู ูโุดููุฏ.

---

## ๐ง ูฺฉุงุช ุงุฏุขูุฑ ููุง

* **ูุฑ ุจุงุฑ ฺฉู ุณุงุช ุฌุฏุฏ ูโุฎุฒุฏ**ุ ูุงุฒู ูุณุช ุงูุฏฺฉุณโุณุงุฒ ูุจู ุฑุง ูพุงฺฉ ฺฉูุฏุ ูโุชูุงูุฏ ูุงูโูุง ุฌุฏุฏ ุฑุง ุงุถุงูู ฺฉูุฏ ู ุฏูุจุงุฑู FAISS ุฑุง ุงุฌุฑุง ฺฉูุฏ.
* **ุงฺฏุฑ ุฎุทุง "no active project" ุฏุฏุฏ**ุ ุนู ุฏุฑ ูุณุฑ ุงุดุชุจุงู ูุณุชุฏ โ ุจุงุฏ ููุงูโุฌุง ุจุงุดุฏ ฺฉู ูุงู `scrapy.cfg` ูุฌูุฏ ุฏุงุฑุฏ.
* **ุงฺฏุฑ Scrapy ุดูุงุฎุชู ููโุดูุฏ**ุ ุจุง ุฏุณุชูุฑ `python -m scrapy crawl partlastic` ุงุฌุฑุง ฺฉูุฏ.
* **ุฏุฑ ููุฏูุฒ**ุ ุงุฒ CMD ูุนููู ุงุณุชูุงุฏู ฺฉูุฏ ูู PowerShell (ุฏุฑ ุจุฑุฎ ูุณุฎูโูุง PATH ุฏุฑุณุช ุชูุธู ููโุดูุฏ).

---

## ๐ ุฎูุงุตู ูุณุฑ ุงุฌุฑุง ฺฉู ูพุฑูฺู

```
(1) ูุนุงูโุณุงุฒ ูุญุท ูุฌุงุฒ
(2) scrapy crawl partlastic   โ ุจุฑุง ุฌูุนโุขูุฑ ุฏุงุฏูโูุง
(3) python advanced_faiss_indexer.py  โ ุจุฑุง ุงูุฏฺฉุณ ฺฉุฑุฏู ูุชูโูุง
(4) ุงุณุชูุงุฏู ุงุฒ index ุฏุฑ ุฏุณุชุงุฑ ููุดููุฏ
```

---

โ๏ธ **ูฺฉุชู ูพุงุงู:**
ุจุฑุง ุณุงุชโูุง ุฏฺฏุฑ ูุฒ ูโุชูุงูุฏ ฺฉ ูุงู spider ุฌุฏุฏ ุฏุฑ ูพูุดูโ `spiders/` ุจุณุงุฒุฏุ
ู ููุท `allowed_domains` ู `start_urls` ุฑุง ุชุบุฑ ุฏูุฏ.
ุจู ุงู ุชุฑุชุจ ูููโ ุฏุงุฏูโูุง ุฏุฑ ฺฉ `documents/` ุฌูุน ูโุดููุฏ ู ุขูุงุฏูโ ุงูุฏฺฉุณ ูุณุชูุฏ.


ู ุชูุงู ฺูุฏู ุงุฏุฑุณ ุจุฑุง ุจุณ ู ูุญู ุฌุณุชุฌู ุฏุงุดุช 
start_urls = [
    "https://partlasticgroup.com/",
    "https://another-site.com/",
    "https://example.ir/"
]

allowed_domains = [
    "partlasticgroup.com",
    "another-site.com",
    "example.ir"
]


