#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Updated RAG System V2 - Compatible with advanced_faiss_indexer1.py
Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ†Ø¯Ú©Ø³ V4 Ø¨Ø±ÙˆØ² Ø´Ø¯Ù‡
"""

import os
import json
import time
import re
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

try:
    from sentence_transformers import SentenceTransformer
    import faiss
    from openai import OpenAI
    print("âœ… All required packages imported successfully")
except ImportError as e:
    print(f"âŒ Missing required package: {e}")
    print("Please install: pip install sentence-transformers faiss-cpu python-dotenv openai")
    exit(1)


@dataclass
class RAGResponse:
    """Structured response from RAG system"""
    query: str
    answer: str
    answer_plain_text: str
    sources: List[Dict[str, Any]]
    confidence: float
    generation_time: float
    retrieval_time: float
    total_time: float
    context_used: str
    category: str
    query_type: str
    related_articles: List[int]


class UniversityRulesRAG:
    """RAG system compatible with advanced_faiss_indexer1.py output"""
    
    def __init__(self, 
                 index_dir: str = "./index_v4_upgraded",
                 env_path: str = "./.env"):
        """Initialize the RAG system with V4 upgraded index"""
        
        load_dotenv(env_path)

        api_key = os.getenv("OPENROUTER_API_KEY") or "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"
        if not api_key:
            raise ValueError("OPENROUTER_API_KEY not found")

        print("ðŸ”‘ API key loaded")

        self.llm_model = os.getenv("LLM_MODEL", "openai/gpt-4o-mini")
        
        try:
            self.openai_client = OpenAI(
                base_url="https://openrouter.ai/api/v1",
                api_key=api_key,
                default_headers={
                    "HTTP-Referer": "https://hadisadoghiyazdi1971.github.io",
                    "X-Title": "FUM University Rules RAG Chatbot",
                }
            )
            _ = self.openai_client.models.list()
            print(f"âœ… Connected to OpenRouter with model: {self.llm_model}")
        except Exception as e:
            print(f"âŒ Error connecting to OpenRouter: {e}")
            raise

        self.index_dir = Path(index_dir)
        
        # Load comprehensive summary from V4 index
        summary_path = self.index_dir / "comprehensive_summary.json"
        if not summary_path.exists():
            raise FileNotFoundError(f"Summary not found: {summary_path}")
        
        with open(summary_path, 'r', encoding='utf-8') as f:
            self.index_summary = json.load(f)
        
        self.embedding_model_name = self.index_summary.get(
            'embedding_model',
            'intfloat/multilingual-e5-large'
        )
        
        print(f"ðŸ¤– Loading embedding model: {self.embedding_model_name}")
        self.embedding_model = SentenceTransformer(self.embedding_model_name, device="cpu")
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        
        # Load FAISS index from V4
        index_path = self.index_dir / "faiss_index_upgraded.index"
        if not index_path.exists():
            raise FileNotFoundError(f"FAISS index not found: {index_path}")
        
        print(f"ðŸ“š Loading FAISS index from: {index_path}")
        self.faiss_index = faiss.read_index(str(index_path))
        print(f"âœ… FAISS index loaded with {self.faiss_index.ntotal} vectors")
        
        # Load metadata from V4
        metadata_path = self.index_dir / "enhanced_metadata.json"
        if not metadata_path.exists():
            raise FileNotFoundError(f"Metadata not found: {metadata_path}")
        
        print(f"ðŸ“š Loading metadata from: {metadata_path}")
        with open(metadata_path, 'r', encoding='utf-8') as f:
            raw_metadata = json.load(f)
        
        # Transform metadata to compatible format
        self.metadata = self._transform_metadata(raw_metadata)
        print(f"âœ… Loaded metadata for {len(self.metadata)} chunks")
        
        self._build_article_index()
        
        self.max_context_length = 3500
        self.max_sources = 5
        
        self.query_patterns = {
            'definitional': [r'Ú†ÛŒØ³Øª', r'ØªØ¹Ø±ÛŒÙ', r'Ù…Ù†Ø¸ÙˆØ± Ø§Ø²', r'Ù…ÙÙ‡ÙˆÙ…', r'ÛŒØ¹Ù†ÛŒ Ú†Ù‡', r'what is', r'define', r'meaning'],
            'procedural': [r'Ú†Ú¯ÙˆÙ†Ù‡', r'Ú†Ø·ÙˆØ±', r'Ù†Ø­ÙˆÙ‡', r'Ø±ÙˆØ´', r'Ù…Ø±Ø§Ø­Ù„', r'Ú¯Ø§Ù…', r'ÙØ±Ø¢ÛŒÙ†Ø¯', r'how to', r'procedure', r'process', r'steps'],
            'eligibility': [r'Ø¢ÛŒØ§ Ù…ÛŒ\s*ØªÙˆØ§Ù†Ù…', r'Ø¢ÛŒØ§ Ù…Ø¬Ø§Ø²', r'Ø´Ø±Ø§ÛŒØ·', r'Ø¶ÙˆØ§Ø¨Ø·', r'Ø§Ù„Ø²Ø§Ù…Ø§Øª', r'can I', r'may I', r'requirements', r'conditions', r'eligible'],
            'numerical': [r'Ú†Ù†Ø¯', r'Ú†Ù‚Ø¯Ø±', r'Ù…ÛŒØ²Ø§Ù†', r'Ø¯Ø±ØµØ¯', r'ØªØ¹Ø¯Ø§Ø¯', r'Ù…Ø¨Ù„Øº', r'how much', r'how many', r'amount', r'percentage'],
            'exception': [r'ØªØ¨ØµØ±Ù‡', r'Ø§Ø³ØªØ«Ù†Ø§', r'Ù…Ú¯Ø±', r'Ø¨Ù‡ Ø¬Ø²', r'Ù…ÙˆØ§Ø±Ø¯ Ø®Ø§Øµ', r'exception', r'special case', r'note'],
            'timeline': [r'Ù…Ù‡Ù„Øª', r'Ø²Ù…Ø§Ù†', r'ØªØ§Ø±ÛŒØ®', r'Ù…Ø¯Øª', r'deadline', r'when', r'time']
        }
        
        print("âœ… RAG system initialized successfully")

    def _transform_metadata(self, raw_metadata: List[Dict]) -> List[Dict]:
        """Transform V4 metadata to compatible format"""
        transformed = []
        
        for chunk in raw_metadata:
            transformed_chunk = {
                'chunk_id': chunk.get('id', ''),
                'chunk_text': chunk.get('text', ''),
                'source_file': chunk.get('source_file', ''),
                'document_title': chunk.get('document_title', ''),
                'document_section': chunk.get('document_section', ''),
                'chunk_type': chunk.get('chunk_type', 'general'),
                'article_numbers': chunk.get('article_numbers', []),
                'note_numbers': chunk.get('note_numbers', []),
                'has_table': chunk.get('has_table', False),
                'has_list': chunk.get('has_list', False),
                'keywords': chunk.get('keywords', []),
                'metadata': chunk.get('metadata', {})
            }
            transformed.append(transformed_chunk)
        
        return transformed

    def _build_article_index(self):
        """Build index for quick article lookup"""
        self.article_to_chunks = {}
        
        for i, meta in enumerate(self.metadata):
            for article_num in meta.get('article_numbers', []):
                if article_num not in self.article_to_chunks:
                    self.article_to_chunks[article_num] = []
                self.article_to_chunks[article_num].append(i)
        
        print(f"ðŸ“‹ Built article index: {len(self.article_to_chunks)} unique articles")
    
    def detect_query_type(self, query: str) -> str:
        """Detect query type"""
        query_lower = query.lower()
        scores = {qtype: 0 for qtype in self.query_patterns.keys()}
        
        for qtype, patterns in self.query_patterns.items():
            for pattern in patterns:
                if re.search(pattern, query_lower):
                    scores[qtype] += 1
        
        max_score = max(scores.values())
        return max(scores, key=scores.get) if max_score > 0 else 'general'
    
    def extract_article_mentions(self, query: str) -> List[int]:
        """Extract mentioned article numbers"""
        article_numbers = []
        patterns = [
            r'Ù…Ø§Ø¯Ù‡\s+(\d+)',
            r'Ø¨Ù†Ø¯\s+(\d+)',
            r'ØªØ¨ØµØ±Ù‡\s+(\d+)'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, query)
            for match in matches:
                try:
                    num = int(match.group(1))
                    if num not in article_numbers:
                        article_numbers.append(num)
                except ValueError:
                    pass
        
        return sorted(article_numbers)
    
    def retrieve_relevant_context(
        self, 
        query: str, 
        top_k: int = 5,
        query_type: str = 'general'
    ) -> Tuple[List[Dict[str, Any]], float, List[int]]:
        """Retrieve relevant context with structure awareness"""
        
        retrieval_start = time.time()
        mentioned_articles = self.extract_article_mentions(query)
        
        query_embedding = self.embedding_model.encode([query])[0].astype('float32')
        
        k_search = min(top_k * 4, self.faiss_index.ntotal)
        distances, indices = self.faiss_index.search(
            query_embedding.reshape(1, -1), 
            k_search
        )
        
        candidate_sources = []
        related_articles = set(mentioned_articles)
        
        for idx, (distance, index) in enumerate(zip(distances[0], indices[0])):
            if index == -1 or index >= len(self.metadata):
                continue
            
            meta = self.metadata[index]
            similarity_score = 1.0 / (1.0 + distance)
            
            boost = 1.0
            chunk_type = meta.get('chunk_type', 'general')
            
            if query_type == 'definitional' and chunk_type == 'definitions':
                boost = 1.3
            elif query_type == 'procedural' and chunk_type == 'article':
                boost = 1.2
            elif query_type == 'numerical' and meta.get('has_table', False):
                boost = 1.3
            elif query_type == 'exception' and meta.get('note_numbers'):
                boost = 1.25
            
            if any(art in mentioned_articles for art in meta.get('article_numbers', [])):
                boost *= 1.5
            
            adjusted_score = similarity_score * boost
            
            source_data = {
                "score": float(adjusted_score),
                "original_score": float(similarity_score),
                "content": meta.get('chunk_text', ''),
                "document_title": meta.get('document_title', ''),
                "document_file": meta.get('source_file', ''),
                "category": meta.get('chunk_type', 'general'),
                "chunk_type": chunk_type,
                "document_section": meta.get('document_section', ''),
                "article_numbers": meta.get('article_numbers', []),
                "note_numbers": meta.get('note_numbers', []),
                "has_table": meta.get('has_table', False),
                "has_list": meta.get('has_list', False),
                "document_id": meta.get('source_file', '').replace('.txt', ''),
                "chunk_id": index,
                "metadata": meta.get('metadata', {})
            }
            
            candidate_sources.append(source_data)
            related_articles.update(meta.get('article_numbers', []))
        
        candidate_sources.sort(key=lambda x: x['score'], reverse=True)
        final_sources = candidate_sources[:top_k]
        
        retrieval_time = time.time() - retrieval_start
        
        print(f"âœ… Retrieved {len(final_sources)} relevant chunks (type: {query_type})")
        
        return final_sources, retrieval_time, sorted(list(related_articles))
    
    def prepare_context_for_llm(
        self, 
        sources: List[Dict[str, Any]],
        query_type: str = 'general'
    ) -> str:
        """Prepare context for LLM"""
        
        if not sources:
            return "Ù‡ÛŒÚ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."
        
        context_parts = []
        total_length = 0
        
        for i, source in enumerate(sources, 1):
            content = source['content']
            document = source['document_title']
            section = source['document_section']
            article_nums = source.get('article_numbers', [])
            note_nums = source.get('note_numbers', [])
            
            metadata_parts = [f"Ù…Ù†Ø¨Ø¹ {i}: {document}"]
            
            if section and section != document:
                metadata_parts.append(f"Ø¨Ø®Ø´: {section}")
            
            if article_nums:
                articles_str = "ØŒ ".join([f"Ù…Ø§Ø¯Ù‡ {num}" for num in article_nums])
                metadata_parts.append(articles_str)
            
            if note_nums:
                notes_str = "ØŒ ".join([f"ØªØ¨ØµØ±Ù‡ {num}" for num in note_nums])
                metadata_parts.append(notes_str)
            
            if source.get('has_table'):
                metadata_parts.append("ðŸ“Š Ø´Ø§Ù…Ù„ Ø¬Ø¯ÙˆÙ„")
            
            metadata_line = " | ".join(metadata_parts)
            source_content = f"[{metadata_line}]\n{content}"
            
            if total_length + len(source_content) > self.max_context_length:
                break
            
            context_parts.append(source_content)
            total_length += len(source_content)
        
        return "\n\n---\n\n".join(context_parts)
    
    def generate_answer_with_llm(
        self, 
        query: str, 
        context: str,
        query_type: str = 'general',
        related_articles: List[int] = None
    ) -> Tuple[str, str, float]:
        """Generate answer using LLM"""
        
        generation_start = time.time()
        
        type_instructions = {
            'definitional': "Û±. ØªØ¹Ø±ÛŒÙ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø±Ø³Ù…ÛŒ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯\nÛ². ØªÙˆØ¶ÛŒØ­ Ù…Ø®ØªØµØ± Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø¨Ø¯Ù‡ÛŒØ¯\nÛ³. Ø´Ø±Ø§ÛŒØ· ÛŒØ§ Ø§Ù„Ø²Ø§Ù…Ø§Øª Ø±Ø§ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯\nÛ´. Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ù…Ø±ØªØ¨Ø· Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ú©Ù†ÛŒØ¯",
            'procedural': "Û±. Ù…Ø±Ø§Ø­Ù„ Ø±Ø§ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯\nÛ². Ù…Ø¯Ø§Ø±Ú© Ùˆ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯\nÛ³. Ù…Ø±Ø¬Ø¹ Ù…Ø³Ø¦ÙˆÙ„ Ø±Ø§ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯\nÛ´. Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ ÛŒØ§ Ù…Ù‡Ù„Øªâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯\nÛµ. Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ø±Ø§ Ø¨Ø±Ø¬Ø³ØªÙ‡ Ú©Ù†ÛŒØ¯",
            'eligibility': "Û±. Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… (Ø¨Ù„Ù‡/Ø®ÛŒØ±) Ø¨Ø¯Ù‡ÛŒØ¯\nÛ². Ø´Ø±Ø§ÛŒØ· Ùˆ Ø§Ù„Ø²Ø§Ù…Ø§Øª Ø±Ø§ Ù„ÛŒØ³Øª Ú©Ù†ÛŒØ¯\nÛ³. Ø§Ø³ØªØ«Ù†Ø§Ù‡Ø§ Ùˆ Ù…ÙˆØ§Ø±Ø¯ Ø®Ø§Øµ Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯\nÛ´. Ø¨Ù‡ Ù…ÙˆØ§Ø¯ Ù…Ø±ØªØ¨Ø· Ø§Ø±Ø¬Ø§Ø¹ Ø¯Ù‚ÛŒÙ‚ Ø¯Ù‡ÛŒØ¯",
            'numerical': "Û±. Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ø¯Ø±ØµØ¯Ù‡Ø§ Ø±Ø§ Ø¨Ø±Ø¬Ø³ØªÙ‡ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯\nÛ². Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¬Ø¯ÙˆÙ„ÛŒ Ø±Ø§ Ø®Ù„Ø§ØµÙ‡ Ú©Ù†ÛŒØ¯\nÛ³. Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯\nÛ´. ÙØ±Ù…ÙˆÙ„ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø±Ø§ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯",
            'exception': "Û±. Ù‚Ø§Ø¹Ø¯Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ Ø±Ø§ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯\nÛ². Ø§Ø³ØªØ«Ù†Ø§Ù‡Ø§ Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯\nÛ³. Ø´Ø±Ø§ÛŒØ· Ø§Ø¹Ù…Ø§Ù„ Ù‡Ø± Ø§Ø³ØªØ«Ù†Ø§ Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯\nÛ´. Ø¨Ù‡ ØªØ¨ØµØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø§Ø±Ø¬Ø§Ø¹ Ø¯Ù‚ÛŒÙ‚ Ø¯Ù‡ÛŒØ¯",
            'timeline': "Û±. Ù…Ù‡Ù„Øªâ€ŒÙ‡Ø§ Ùˆ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ ÙˆØ§Ø¶Ø­ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯\nÛ². Ù†Ù‚Ø§Ø· Ø´Ø±ÙˆØ¹ Ùˆ Ù¾Ø§ÛŒØ§Ù† Ù…Ù‡Ù„Øªâ€ŒÙ‡Ø§ Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯\nÛ³. ØªØ¨Ø¹Ø§Øª Ø¹Ø¯Ù… Ø±Ø¹Ø§ÛŒØª Ù…Ù‡Ù„Øª Ø±Ø§ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯\nÛ´. Ù…ÙˆØ§Ø±Ø¯ ØªÙ…Ø¯ÛŒØ¯ ÛŒØ§ Ø§Ø³ØªØ«Ù†Ø§ Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯",
            'general': "Û±. Ù¾Ø§Ø³Ø® Ø¬Ø§Ù…Ø¹ Ùˆ Ú©Ø§Ù…Ù„ Ø¨Ø¯Ù‡ÛŒØ¯\nÛ². Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ø±Ø§ Ø¨Ø±Ø¬Ø³ØªÙ‡ Ú©Ù†ÛŒØ¯\nÛ³. Ø¨Ù‡ Ù…ÙˆØ§Ø¯ Ùˆ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø§Ø±Ø¬Ø§Ø¹ Ø¯Ù‡ÛŒØ¯\nÛ´. Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ù…Ø«Ø§Ù„ ÛŒØ§ ØªÙˆØ¶ÛŒØ­ ØªÚ©Ù…ÛŒÙ„ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯"
        }
        
        specific_instruction = type_instructions.get(query_type, type_instructions['general'])
        
        article_context = ""
        if related_articles:
            articles_list = "ØŒ ".join([f"Ù…Ø§Ø¯Ù‡ {num}" for num in related_articles])
            article_context = f"\n\nÙ…ÙˆØ§Ø¯Ù…Ø±ØªØ¨Ø· Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡: {articles_list}"
        
        prompt = f"""Ø´Ù…Ø§ ÛŒÚ© Ù…Ø´Ø§ÙˆØ± Ù‚ÙˆØ§Ù†ÛŒÙ† Ùˆ Ù…Ù‚Ø±Ø±Ø§Øª  Ø´Ø±Ú©Øª Ø¨Ø±Ù‚  Ù‡Ø³ØªÛŒØ¯. Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ØŒ Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø± Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø¨Ø¯Ù‡ÛŒØ¯.

Ù†ÙˆØ¹ Ø³Ø¤Ø§Ù„: {query_type}
{article_context}

Ù…Ù‚Ø±Ø±Ø§Øª Ø´Ø±Ú©Øª Ø¨Ø±Ù‚:
{context}

Ø³Ø¤Ø§Ù„ Ú©Ø§Ø±Ø¨Ø±: {query}

Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ Ù¾Ø§Ø³Ø®:
{specific_instruction}

Ù†Ú©Ø§Øª Ù…Ù‡Ù…:
- Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ùˆ Ø±Ø³Ù…ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯
- Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ Ø§Ø±Ø¬Ø§Ø¹ Ø¯Ù‚ÛŒÙ‚ Ø¯Ù‡ÛŒØ¯
- Ø¯Ø± ØµÙˆØ±Øª Ø§Ø¨Ù‡Ø§Ù… ÛŒØ§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ø¨ÛŒØ´ØªØ±ØŒ ØµØ±Ø§Ø­Øªâ€ŒØ¢Ù…ÛŒØ² Ø§Ø¹Ù„Ø§Ù… Ú©Ù†ÛŒØ¯
- Ø§Ú¯Ø± Ú†Ù†Ø¯ Ø­Ø§Ù„Øª ÛŒØ§ Ø´Ø±Ø· Ù…Ø®ØªÙ„Ù ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ù‡Ù…Ù‡ Ø±Ø§ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯
- Ø§Ø² Ø³Ø§Ø®ØªØ§Ø± ÙˆØ§Ø¶Ø­ Ùˆ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ø¹Ù†ÙˆØ§Ù†â€ŒÙ‡Ø§ØŒ ÙÙ‡Ø±Ø³ØªØŒ Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ)

Ù¾Ø§Ø³Ø®:"""

        try:
            response = self.openai_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://github.com/FUM_RAG",
                    "X-Title": "FUM RAG System",
                },
                model=self.llm_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            
            answer_markdown = response.choices[0].message.content
            answer_plain = answer_markdown.replace('**', '').replace('*', '')
            answer_plain = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', answer_plain)
            
            generation_time = time.time() - generation_start
            return answer_markdown, answer_plain, generation_time
            
        except Exception as e:
            generation_time = time.time() - generation_start
            error_answer = f"Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± ØªÙˆÙ„ÙŠØ¯ Ù¾Ø§Ø³Ø® Ø±Ø® Ø¯Ø§Ø¯: {str(e)}\n\nØ¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù‚Ø±Ø±Ø§Øª Ù…ÙˆØ¬ÙˆØ¯:\n{context[:500]}..."
            return error_answer, error_answer, generation_time
    
    def answer_question(self, query: str, top_k: int = 5) -> RAGResponse:
        """Complete RAG pipeline"""
        
        total_start = time.time()
        
        print(f"ðŸ” Processing query: {query}")
        
        query_type = self.detect_query_type(query)
        print(f"ðŸŽ¯ Query type: {query_type}")
        
        sources, retrieval_time, related_articles = self.retrieve_relevant_context(
            query, top_k, query_type
        )
        
        if not sources:
            total_time = time.time() - total_start
            return RAGResponse(
                query=query,
                answer="Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø³Ø¤Ø§Ù„ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§ Ø³Ø¤Ø§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù†ÛŒØ¯.",
                answer_plain_text="Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯",
                sources=[],
                confidence=0.0,
                generation_time=0.0,
                retrieval_time=retrieval_time,
                total_time=total_time,
                context_used="",
                category="Ù†Ø§Ù…Ø´Ø®Øµ",
                query_type=query_type,
                related_articles=related_articles
            )
        
        context = self.prepare_context_for_llm(sources, query_type)
        answer_markdown, answer_plain, generation_time = self.generate_answer_with_llm(
            query, context, query_type, related_articles
        )
        
        total_time = time.time() - total_start
        avg_confidence = (sum(source['score'] for source in sources) / len(sources)) if sources else 0.0
        primary_category = sources[0]['category'] if sources else "Ù†Ø§Ù…Ø´Ø®Øµ"
        
        source_info = []
        for source in sources:
            source_info.append({
                "document": source['document_title'],
                "section": source.get('document_section', ''),
                "category": source['category'],
                "confidence": source['score'],
                "chunk_type": source.get('chunk_type', 'general'),
                "article_numbers": source.get('article_numbers', []),
                "note_numbers": source.get('note_numbers', []),
                "has_table": source.get('has_table', False),
                "has_list": source.get('has_list', False)
            })
        
        print(f"âœ… Response generated in {total_time:.2f}s")
        
        return RAGResponse(
            query=query,
            answer=answer_markdown,
            answer_plain_text=answer_plain,
            sources=source_info,
            confidence=avg_confidence,
            generation_time=generation_time,
            retrieval_time=retrieval_time,
            total_time=total_time,
            context_used=context,
            category=primary_category,
            query_type=query_type,
            related_articles=related_articles
        )
    
    def get_system_statistics(self) -> Dict[str, Any]:
        """Get system statistics"""
        
        stats = {
            "index_type": "FAISS",
            "total_chunks": len(self.metadata),
            "total_documents": self.faiss_index.ntotal,
            "embedding_model": self.embedding_model_name,
            "embedding_dimension": self.embedding_dim,
            "llm_model": self.llm_model,
            "max_context_length": self.max_context_length,
            "total_articles": len(self.article_to_chunks),
            "indexed_at": self.index_summary.get('indexed_at', 'unknown'),
            "chunking_strategy": self.index_summary.get('chunking_strategy', 'unknown'),
            "status": "active"
        }
        
        return stats