#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FAISS Index Creator V3 - Advanced RAG Optimized
Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ RAG
"""

import os
import json
import re
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime
from dataclasses import dataclass, asdict
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    from sentence_transformers import SentenceTransformer
    import faiss
    import numpy as np
    from sklearn.preprocessing import normalize
    print("âœ… Ù‡Ù…Ù‡ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù†Ø¯")
except ImportError as e:
    print(f"âŒ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ù…ÙÙ‚ÙˆØ¯Ù‡: {e}")
    print("Ù„Ø·ÙØ§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯: pip install sentence-transformers faiss-cpu scikit-learn")
    exit(1)

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class AdvancedDocumentChunk:
    """Ù‚Ø·Ø¹Ù‡ Ø³Ù†Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ ØºÙ†ÛŒ"""
    id: str
    text: str
    source_file: str
    chunk_type: str  # 'title', 'introduction', 'article', 'definitions', 'table', 'conclusion', 'regulation'
    article_numbers: List[int]
    note_numbers: List[int]
    has_table: bool
    has_list: bool
    document_section: str
    document_title: str
    chunk_index: int
    embedding: Optional[np.ndarray] = None
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
        if not self.id:
            self.id = self.generate_id()
    
    def generate_id(self) -> str:
        """Ø§ÛŒØ¬Ø§Ø¯ Ø´Ù†Ø§Ø³Ù‡ ÛŒÚ©ØªØ§ Ø¨Ø±Ø§ÛŒ Ù‚Ø·Ø¹Ù‡"""
        content = f"{self.source_file}_{self.chunk_index}_{self.text[:50]}"
        return hashlib.md5(content.encode()).hexdigest()

class AdvancedStructureAwareChunker:
    """Ù‚Ø·Ø¹Ù‡â€ŒØ¨Ù†Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ø¯Ø±Ú© Ø³Ø§Ø®ØªØ§Ø± Ø³Ù†Ø¯"""
    
    def __init__(self, max_chunk_size: int = 800, min_chunk_size: int = 150, overlap_size: int = 50):
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = min_chunk_size
        self.overlap_size = overlap_size
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³Ø§Ø®ØªØ§Ø±
        self.patterns = {
            'title': re.compile(r'^#\s+(.+)$', re.MULTILINE),
            'section': re.compile(r'^##\s+(.+)$', re.MULTILINE),
            'subsection': re.compile(r'^###\s+(.+)$', re.MULTILINE),
            'article': re.compile(r'##\s*Ù…Ø§Ø¯Ù‡\s+(\d+)[:-]?\s*(.+?)(?=\n|$)', re.MULTILINE),
            'note': re.compile(r'\*\*ØªØ¨ØµØ±Ù‡\s+(\d+):\*\*\s*(.+?)(?=\n\n|\*\*ØªØ¨ØµØ±Ù‡|\Z)', re.DOTALL),
            'regulation': re.compile(r'##\s*(Ø¢ÛŒÛŒÙ† Ù†Ø§Ù…Ù‡|Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„|Ø¨Ø®Ø´Ù†Ø§Ù…Ù‡)\s*(.+?)(?=\n|$)', re.MULTILINE),
            'definition': re.compile(r'\*\*([^*]+):\*\*\s*(.+?)(?=\n-|\n\*\*|\n\n|\Z)', re.DOTALL),
            'list_item': re.compile(r'^-\s+(.+)$', re.MULTILINE),
            'table': re.compile(r'\|.+\|', re.MULTILINE),
            'numbered_list': re.compile(r'^\d+[-.)]\s+(.+)$', re.MULTILINE),
            'reference': re.compile(r'(Ø¨Ù‡ Ø§Ø³ØªÙ†Ø§Ø¯|Ù…ÙˆØ¶ÙˆØ¹|Ø·Ø¨Ù‚)\s+Ù…Ø§Ø¯Ù‡\s+(\d+)', re.IGNORECASE),
        }
    
    def extract_document_structure(self, text: str) -> Dict[str, Any]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø§Ø®ØªØ§Ø± Ú©Ø§Ù…Ù„ Ø³Ù†Ø¯"""
        structure = {
            'title': self._extract_title(text),
            'sections': self._extract_sections(text),
            'articles': self._extract_articles(text),
            'notes': self._extract_notes(text),
            'regulations': self._extract_regulations(text),
            'tables': self._extract_tables(text)
        }
        return structure
    
    def _extract_title(self, text: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ"""
        match = self.patterns['title'].search(text)
        return match.group(1).strip() if match else "Ø¹Ù†ÙˆØ§Ù† Ù†Ø§Ù…Ø´Ø®Øµ"
    
    def _extract_sections(self, text: str) -> List[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø³Ù†Ø¯"""
        sections = []
        for match in self.patterns['section'].finditer(text):
            sections.append({
                'title': match.group(1).strip(),
                'start_pos': match.start(),
                'end_pos': match.end()
            })
        return sections
    
    def _extract_articles(self, text: str) -> List[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ§Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÛŒ"""
        articles = []
        for match in self.patterns['article'].finditer(text):
            articles.append({
                'number': int(match.group(1)),
                'title': match.group(2).strip(),
                'content_start': match.end()
            })
        return articles
    
    def _extract_notes(self, text: str) -> List[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ¨ØµØ±Ù‡â€ŒÙ‡Ø§"""
        notes = []
        for match in self.patterns['note'].finditer(text):
            notes.append({
                'number': int(match.group(1)),
                'content': match.group(2).strip()
            })
        return notes
    
    def _extract_regulations(self, text: str) -> List[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡â€ŒÙ‡Ø§ Ùˆ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„â€ŒÙ‡Ø§"""
        regulations = []
        for match in self.patterns['regulation'].finditer(text):
            regulations.append({
                'type': match.group(1).strip(),
                'title': match.group(2).strip()
            })
        return regulations
    
    def _extract_tables(self, text: str) -> List[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø¯Ø§ÙˆÙ„"""
        tables = []
        for match in self.patterns['table'].finditer(text):
            tables.append({
                'content': match.group(0),
                'start_pos': match.start()
            })
        return tables
    
    def intelligent_chunking(self, text: str, source_file: str) -> List[AdvancedDocumentChunk]:
        """Ù‚Ø·Ø¹Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ø­ÙØ¸ Ø²Ù…ÛŒÙ†Ù‡"""
        chunks = []
        document_title = self._extract_title(text)
        structure = self.extract_document_structure(text)
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø®ØªØ§Ø±
        current_pos = 0
        chunk_index = 0
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¹Ù†ÙˆØ§Ù† Ùˆ Ù…Ù‚Ø¯Ù…Ù‡
        title_chunk = self._create_title_chunk(text, source_file, document_title, chunk_index)
        if title_chunk:
            chunks.append(title_chunk)
            chunk_index += 1
            current_pos = len(title_chunk.text)
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø®Ø´â€ŒÙ‡Ø§
        for section in structure['sections']:
            section_text = self._extract_section_text(text, section['start_pos'])
            section_chunks = self._process_section(section_text, source_file, document_title, 
                                                 section['title'], chunk_index)
            chunks.extend(section_chunks)
            chunk_index += len(section_chunks)
        
        # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‚Ø·Ø¹Ø§Øª Ú©ÙˆÚ†Ú©
        chunks = self._optimize_small_chunks(chunks)
        
        return chunks
    
    def _create_title_chunk(self, text: str, source_file: str, document_title: str, chunk_index: int) -> Optional[AdvancedDocumentChunk]:
        """Ø§ÛŒØ¬Ø§Ø¯ Ù‚Ø·Ø¹Ù‡ Ø¹Ù†ÙˆØ§Ù†"""
        title_match = self.patterns['title'].search(text)
        if not title_match:
            return None
        
        title_text = title_match.group(0)
        remaining = text[title_match.end():].strip()
        
        # Ø§Ú¯Ø± Ù…Ù‚Ø¯Ù…Ù‡ Ú©ÙˆØªØ§Ù‡ Ø§Ø³ØªØŒ Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† Ø§Ø¯ØºØ§Ù… Ú©Ù†
        if remaining and len(remaining) < 300:
            combined_text = f"{title_text}\n\n{remaining}"
            return AdvancedDocumentChunk(
                id="",
                text=combined_text,
                source_file=source_file,
                chunk_type="title_intro",
                document_title=document_title,
                document_section="Ø¹Ù†ÙˆØ§Ù† Ùˆ Ù…Ù‚Ø¯Ù…Ù‡",
                chunk_index=chunk_index,
                article_numbers=[],
                note_numbers=[],
                has_table=False,
                has_list=False,
                metadata={'combined': True}
            )
        else:
            return AdvancedDocumentChunk(
                id="",
                text=title_text,
                source_file=source_file,
                chunk_type="title",
                document_title=document_title,
                document_section="Ø¹Ù†ÙˆØ§Ù†",
                chunk_index=chunk_index,
                article_numbers=[],
                note_numbers=[],
                has_table=False,
                has_list=False
            )
    
    def _extract_section_text(self, text: str, start_pos: int) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø®Ø´"""
        next_section = re.search(r'\n##\s+', text[start_pos:])
        if next_section:
            return text[start_pos:start_pos + next_section.start()]
        return text[start_pos:]
    
    def _process_section(self, section_text: str, source_file: str, document_title: str, 
                        section_name: str, start_index: int) -> List[AdvancedDocumentChunk]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ø¨Ø®Ø´"""
        chunks = []
        
        # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø¨Ø®Ø´
        section_type = self._classify_section_type(section_text, section_name)
        
        if section_type == "article_section":
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø®Ø´ Ù…ÙˆØ§Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÛŒ
            chunks.extend(self._process_article_section(section_text, source_file, document_title, 
                                                      section_name, start_index))
        elif section_type == "regulation":
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡â€ŒÙ‡Ø§
            chunks.extend(self._process_regulation_section(section_text, source_file, document_title, 
                                                         section_name, start_index))
        else:
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø®Ø´ Ø¹Ø§Ø¯ÛŒ
            if len(section_text) <= self.max_chunk_size:
                chunks.append(self._create_chunk(section_text, source_file, document_title, 
                                               section_name, section_type, start_index))
            else:
                # ØªÙ‚Ø³ÛŒÙ… Ø¨Ø®Ø´ Ø¨Ø²Ø±Ú¯
                sub_chunks = self._split_large_section(section_text, source_file, document_title, 
                                                     section_name, section_type, start_index)
                chunks.extend(sub_chunks)
        
        return chunks
    
    def _classify_section_type(self, text: str, section_name: str) -> str:
        """Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†ÙˆØ¹ Ø¨Ø®Ø´"""
        text_lower = text.lower()
        section_lower = section_name.lower()
        
        if any(word in section_lower for word in ['Ù…Ø§Ø¯Ù‡', 'Ø§ØµÙ„', 'Ù…Ø§Ø¯Û€']):
            return "article_section"
        elif any(word in section_lower for word in ['Ø¢ÛŒÛŒÙ†', 'Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„', 'Ø¨Ø®Ø´Ù†Ø§Ù…Ù‡']):
            return "regulation"
        elif any(word in section_lower for word in ['ØªØ¹Ø±ÛŒÙ', 'ØªØ¹Ø§Ø±ÛŒÙ']):
            return "definitions"
        elif any(word in section_lower for word in ['Ø¬Ø¯ÙˆÙ„', 'ÙØ±Ù…']):
            return "table_section"
        elif any(word in section_lower for word in ['Ù…Ù‚Ø¯Ù…Ù‡', 'Ù¾ÛŒØ´Ú¯ÙØªØ§Ø±']):
            return "introduction"
        elif any(word in section_lower for word in ['Ù†ØªÛŒØ¬Ù‡', 'Ù¾Ø§ÛŒØ§Ù†', 'ØªØµÙˆÛŒØ¨']):
            return "conclusion"
        else:
            return "general"
    
    def _process_article_section(self, section_text: str, source_file: str, document_title: str, 
                               section_name: str, start_index: int) -> List[AdvancedDocumentChunk]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø®Ø´ Ù…ÙˆØ§Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÛŒ"""
        chunks = []
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ÙˆØ§Ø¯ Ùˆ ØªØ¨ØµØ±Ù‡â€ŒÙ‡Ø§
        parts = re.split(r'(\*\*ØªØ¨ØµØ±Ù‡\s+\d+:\*\*)', section_text)
        
        current_chunk = ""
        current_index = start_index
        
        for part in parts:
            part = part.strip()
            if not part:
                continue
            
            if part.startswith('**ØªØ¨ØµØ±Ù‡'):
                # Ø§Ú¯Ø± ØªØ¨ØµØ±Ù‡ Ø§Ø³Øª
                if current_chunk and len(current_chunk) + len(part) < self.max_chunk_size:
                    # Ø§Ø¯ØºØ§Ù… Ø¨Ø§ Ù‚Ø·Ø¹Ù‡ Ù‚Ø¨Ù„ÛŒ
                    current_chunk += f"\n\n{part}"
                else:
                    # Ø°Ø®ÛŒØ±Ù‡ Ù‚Ø·Ø¹Ù‡ Ù‚Ø¨Ù„ÛŒ Ùˆ Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÛŒØ¯
                    if current_chunk:
                        chunks.append(self._create_chunk(current_chunk, source_file, document_title, 
                                                       section_name, "article", current_index))
                        current_index += 1
                    current_chunk = part
            else:
                # Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ù…Ø§Ø¯Ù‡
                if current_chunk:
                    current_chunk += f"\n\n{part}"
                else:
                    current_chunk = part
            
            # Ø§Ú¯Ø± Ù‚Ø·Ø¹Ù‡ Ø¨Ø²Ø±Ú¯ Ø´Ø¯ØŒ Ø°Ø®ÛŒØ±Ù‡ Ú©Ù†
            if len(current_chunk) >= self.max_chunk_size:
                chunks.append(self._create_chunk(current_chunk, source_file, document_title, 
                                               section_name, "article", current_index))
                current_index += 1
                current_chunk = ""
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù‚Ø·Ø¹Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
        if current_chunk:
            chunks.append(self._create_chunk(current_chunk, source_file, document_title, 
                                           section_name, "article", current_index))
        
        return chunks
    
    def _process_regulation_section(self, section_text: str, source_file: str, document_title: str, 
                                  section_name: str, start_index: int) -> List[AdvancedDocumentChunk]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø®Ø´ Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡â€ŒÙ‡Ø§"""
        # Ø¨Ø±Ø§ÛŒ Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡â€ŒÙ‡Ø§ Ø§Ø² Ù‚Ø·Ø¹Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        return self._split_by_logical_units(section_text, source_file, document_title, 
                                          section_name, "regulation", start_index)
    
    def _split_by_logical_units(self, text: str, source_file: str, document_title: str, 
                              section_name: str, chunk_type: str, start_index: int) -> List[AdvancedDocumentChunk]:
        """ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ§Ø­Ø¯Ù‡Ø§ÛŒ Ù…Ù†Ø·Ù‚ÛŒ"""
        chunks = []
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø·Ù‚ÛŒ
        paragraphs = re.split(r'\n\s*\n', text)
        
        current_chunk = ""
        current_index = start_index
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # Ø§Ú¯Ø± Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ Ø§Ø³ØªØŒ Ø®ÙˆØ¯Ø´ ÛŒÚ© Ù‚Ø·Ø¹Ù‡ Ø´ÙˆØ¯
            if len(para) > self.max_chunk_size * 0.7:
                if current_chunk:
                    chunks.append(self._create_chunk(current_chunk, source_file, document_title, 
                                                   section_name, chunk_type, current_index))
                    current_index += 1
                    current_chunk = ""
                
                # ØªÙ‚Ø³ÛŒÙ… Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø¨Ø²Ø±Ú¯
                sub_chunks = self._split_large_paragraph(para, source_file, document_title, 
                                                       section_name, chunk_type, current_index)
                chunks.extend(sub_chunks)
                current_index += len(sub_chunks)
            else:
                # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù…Ú©Ø§Ù† Ø§Ø¯ØºØ§Ù…
                if len(current_chunk) + len(para) + 2 <= self.max_chunk_size:
                    if current_chunk:
                        current_chunk += f"\n\n{para}"
                    else:
                        current_chunk = para
                else:
                    # Ø°Ø®ÛŒØ±Ù‡ Ù‚Ø·Ø¹Ù‡ ÙØ¹Ù„ÛŒ
                    if current_chunk:
                        chunks.append(self._create_chunk(current_chunk, source_file, document_title, 
                                                       section_name, chunk_type, current_index))
                        current_index += 1
                    current_chunk = para
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù‚Ø·Ø¹Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
        if current_chunk:
            chunks.append(self._create_chunk(current_chunk, source_file, document_title, 
                                           section_name, chunk_type, current_index))
        
        return chunks
    
    def _split_large_section(self, section_text: str, source_file: str, document_title: str, 
                           section_name: str, chunk_type: str, start_index: int) -> List[AdvancedDocumentChunk]:
        """ØªÙ‚Ø³ÛŒÙ… Ø¨Ø®Ø´ Ø¨Ø²Ø±Ú¯"""
        return self._split_by_logical_units(section_text, source_file, document_title, 
                                          section_name, chunk_type, start_index)
    
    def _split_large_paragraph(self, paragraph: str, source_file: str, document_title: str, 
                             section_name: str, chunk_type: str, start_index: int) -> List[AdvancedDocumentChunk]:
        """ØªÙ‚Ø³ÛŒÙ… Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø¨Ø²Ø±Ú¯"""
        chunks = []
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¬Ù…Ù„Ø§Øª
        sentences = re.split(r'[.!?]+\s+', paragraph)
        
        current_chunk = ""
        current_index = start_index
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if len(current_chunk) + len(sentence) + 2 <= self.max_chunk_size:
                if current_chunk:
                    current_chunk += f". {sentence}"
                else:
                    current_chunk = sentence
            else:
                if current_chunk:
                    chunks.append(self._create_chunk(current_chunk, source_file, document_title, 
                                                   section_name, chunk_type, current_index))
                    current_index += 1
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(self._create_chunk(current_chunk, source_file, document_title, 
                                           section_name, chunk_type, current_index))
        
        return chunks
    
    def _create_chunk(self, text: str, source_file: str, document_title: str, 
                     section_name: str, chunk_type: str, chunk_index: int) -> AdvancedDocumentChunk:
        """Ø§ÛŒØ¬Ø§Ø¯ Ù‚Ø·Ø¹Ù‡ Ø³Ù†Ø¯"""
        
        article_numbers = self._extract_article_numbers(text)
        note_numbers = self._extract_note_numbers(text)
        has_table = bool(self.patterns['table'].search(text))
        has_list = bool(self.patterns['list_item'].search(text) or 
                       self.patterns['numbered_list'].search(text))
        
        metadata = {
            'has_definitions': '**' in text and ':' in text,
            'has_references': bool(self.patterns['reference'].search(text)),
            'sentence_count': len(re.findall(r'[.!?]+', text)),
            'word_count': len(text.split()),
            'char_count': len(text),
            'quality_score': self._calculate_quality_score(text)
        }
        
        return AdvancedDocumentChunk(
            id="",
            text=text,
            source_file=source_file,
            chunk_type=chunk_type,
            document_title=document_title,
            document_section=section_name,
            chunk_index=chunk_index,
            article_numbers=article_numbers,
            note_numbers=note_numbers,
            has_table=has_table,
            has_list=has_list,
            metadata=metadata
        )
    
    def _extract_article_numbers(self, text: str) -> List[int]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ù…Ø§Ø±Ù‡ Ù…ÙˆØ§Ø¯"""
        numbers = []
        for match in self.patterns['article'].finditer(text):
            try:
                numbers.append(int(match.group(1)))
            except (ValueError, IndexError):
                pass
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª Ø¨Ù‡ Ù…ÙˆØ§Ø¯
        for match in self.patterns['reference'].finditer(text):
            try:
                num = int(match.group(2))
                if num not in numbers:
                    numbers.append(num)
            except (ValueError, IndexError):
                pass
        
        return sorted(list(set(numbers)))
    
    def _extract_note_numbers(self, text: str) -> List[int]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ù…Ø§Ø±Ù‡ ØªØ¨ØµØ±Ù‡â€ŒÙ‡Ø§"""
        numbers = []
        for match in self.patterns['note'].finditer(text):
            try:
                numbers.append(int(match.group(1)))
            except (ValueError, IndexError):
                pass
        return sorted(numbers)
    
    def _calculate_quality_score(self, text: str) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª Ù‚Ø·Ø¹Ù‡"""
        score = 0.0
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„
        if self.min_chunk_size <= len(text) <= self.max_chunk_size:
            score += 0.3
        elif len(text) > 50:  # Ø­Ø¯Ø§Ù‚Ù„ Ø·ÙˆÙ„ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„
            score += 0.1
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø®ØªØ§Ø±
        if any(marker in text for marker in ['**', '##', 'Ù…Ø§Ø¯Ù‡', 'ØªØ¨ØµØ±Ù‡']):
            score += 0.3
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ø§Ù…Ù„ Ø¨ÙˆØ¯Ù† Ø¬Ù…Ù„Ø§Øª
        sentences = re.findall(r'[^.!?]+[.!?]', text)
        if len(sentences) >= 1:
            score += 0.2
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙ†ÙˆØ¹ Ù…Ø­ØªÙˆØ§
        words = text.split()
        unique_words = len(set(words))
        if len(words) > 0:
            diversity = unique_words / len(words)
            score += diversity * 0.2
        
        return min(score, 1.0)
    
    def _optimize_small_chunks(self, chunks: List[AdvancedDocumentChunk]) -> List[AdvancedDocumentChunk]:
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‚Ø·Ø¹Ø§Øª Ú©ÙˆÚ†Ú©"""
        if not chunks:
            return chunks
        
        optimized = []
        i = 0
        
        while i < len(chunks):
            current = chunks[i]
            
            # Ø§Ú¯Ø± Ù‚Ø·Ø¹Ù‡ Ø®ÛŒÙ„ÛŒ Ú©ÙˆÚ†Ú© Ø§Ø³Øª Ùˆ Ù‚Ø·Ø¹Ù‡ Ø¨Ø¹Ø¯ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
            if (len(current.text) < self.min_chunk_size and 
                i < len(chunks) - 1 and 
                chunks[i+1].chunk_type == current.chunk_type):
                
                next_chunk = chunks[i+1]
                combined_text = f"{current.text}\n\n{next_chunk.text}"
                
                if len(combined_text) <= self.max_chunk_size:
                    # Ø§Ø¯ØºØ§Ù… Ù‚Ø·Ø¹Ø§Øª
                    merged_chunk = AdvancedDocumentChunk(
                        id="",
                        text=combined_text,
                        source_file=current.source_file,
                        chunk_type=current.chunk_type,
                        document_title=current.document_title,
                        document_section=current.document_section,
                        chunk_index=current.chunk_index,
                        article_numbers=list(set(current.article_numbers + next_chunk.article_numbers)),
                        note_numbers=list(set(current.note_numbers + next_chunk.note_numbers)),
                        has_table=current.has_table or next_chunk.has_table,
                        has_list=current.has_list or next_chunk.has_list,
                        metadata={
                            'merged': True,
                            'original_chunks': [current.chunk_index, next_chunk.chunk_index],
                            'quality_score': (current.metadata.get('quality_score', 0) + 
                                            next_chunk.metadata.get('quality_score', 0)) / 2
                        }
                    )
                    optimized.append(merged_chunk)
                    i += 2  # Ø¯Ùˆ Ù‚Ø·Ø¹Ù‡ Ø±Ø§ Ù¾Ø±Ø´ Ú©Ù†
                    continue
            
            optimized.append(current)
            i += 1
        
        return optimized

class AdvancedFAISSIndexCreator:
    """Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†Ù†Ø¯Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    
    def __init__(
        self,
        documents_dir: str = "./documents",
        index_dir: str = "./index_v3",
        embedding_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        max_chunk_size: int = 800,
        min_chunk_size: int = 150
    ):
        self.documents_dir = Path(documents_dir)
        self.index_dir = Path(index_dir)
        self.embedding_model_name = embedding_model
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³
        self.index_dir.mkdir(parents=True, exist_ok=True)
        
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øªâ€ŒÙ‡Ø§
        logger.info(f"ğŸ¤– Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ embedding: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model, device="cpu")
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        
        self.chunker = AdvancedStructureAwareChunker(
            max_chunk_size=max_chunk_size,
            min_chunk_size=min_chunk_size
        )
        
        # Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ
        self.all_chunks: List[AdvancedDocumentChunk] = []
        self.embeddings: np.ndarray = None
    
    def process_documents_parallel(self, max_workers: int = 4):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ Ø§Ø³Ù†Ø§Ø¯"""
        logger.info(f"ğŸ“‚ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø³Ù†Ø§Ø¯ Ø§Ø²: {self.documents_dir}")
        
        txt_files = list(self.documents_dir.glob("*.txt"))
        
        if not txt_files:
            raise FileNotFoundError(f"Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ .txt Ø¯Ø± {self.documents_dir} ÛŒØ§ÙØª Ù†Ø´Ø¯")
        
        logger.info(f"ğŸ“„ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ù†Ø§Ø¯ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(txt_files)}")
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_file = {
                executor.submit(self._process_single_document, file_path): file_path 
                for file_path in sorted(txt_files)
            }
            
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    chunks = future.result()
                    logger.info(f"âœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„: {file_path.name} - {len(chunks)} Ù‚Ø·Ø¹Ù‡")
                except Exception as exc:
                    logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {file_path.name}: {exc}")
        
        logger.info(f"âœ… Ù…Ø¬Ù…ÙˆØ¹ Ù‚Ø·Ø¹Ø§Øª Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡: {len(self.all_chunks)}")
        
        # Ø¢Ù…Ø§Ø±
        self._print_statistics()
    
    def _process_single_document(self, file_path: Path) -> List[AdvancedDocumentChunk]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ø³Ù†Ø¯"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ù‚Ø·Ø¹Ø§Øª
            chunks = self.chunker.intelligent_chunking(text, file_path.name)
            
            # Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ú©Ù„ÛŒ
            self.all_chunks.extend(chunks)
            
            return chunks
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {file_path}: {e}")
            return []
    
    def _print_statistics(self):
        """Ú†Ø§Ù¾ Ø¢Ù…Ø§Ø± Ù‚Ø·Ø¹Ø§Øª"""
        chunk_types = {}
        quality_scores = []
        
        for chunk in self.all_chunks:
            chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1
            quality_scores.append(chunk.metadata.get('quality_score', 0))
        
        logger.info("\nğŸ“Š ØªÙˆØ²ÛŒØ¹ Ù‚Ø·Ø¹Ø§Øª:")
        for chunk_type, count in sorted(chunk_types.items()):
            percentage = (count / len(self.all_chunks)) * 100
            logger.info(f"   {chunk_type}: {count} ({percentage:.1f}%)")
        
        if quality_scores:
            avg_quality = sum(quality_scores) / len(quality_scores)
            logger.info(f"   Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª: {avg_quality:.2f}")
    
    def create_embeddings_batch(self, batch_size: int = 64):
        """Ø§ÛŒØ¬Ø§Ø¯ embedding Ø¨Ù‡ ØµÙˆØ±Øª batch"""
        logger.info(f"ğŸ§  Ø§ÛŒØ¬Ø§Ø¯ embedding Ø¨Ø±Ø§ÛŒ {len(self.all_chunks)} Ù‚Ø·Ø¹Ù‡...")
        
        texts = [chunk.text for chunk in self.all_chunks]
        
        # Ø§ÛŒØ¬Ø§Ø¯ embedding Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù…ØªÙ†â€ŒÙ‡Ø§
        all_embeddings = self.embedding_model.encode(
            texts, 
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ embeddingÙ‡Ø§
        self.embeddings = normalize(all_embeddings, norm='l2', axis=1)
        
        logger.info(f"âœ… Ø§ÛŒØ¬Ø§Ø¯ {len(self.embeddings)} embedding Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
    
    def create_optimized_faiss_index(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡"""
        logger.info("ğŸ“Š Ø§ÛŒØ¬Ø§Ø¯ Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS...")
        
        if self.embeddings is None:
            raise ValueError("Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ÛŒØ¯ embeddingÙ‡Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ø´ÙˆÙ†Ø¯")
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø§ÛŒÙ†Ø¯Ú©Ø³ HNSW Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø§ÛŒÛŒ Ø¨Ù‡ØªØ±
        index = faiss.IndexHNSWFlat(self.embedding_dim, 32)
        
        # Ø§ÙØ²ÙˆØ¯Ù† embeddingÙ‡Ø§ Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³
        index.add(self.embeddings.astype('float32'))
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³
        index_path = self.index_dir / "faiss_index_optimized.bin"
        faiss.write_index(index, str(index_path))
        logger.info(f"âœ… Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {index_path}")
        
        return index
    
    def save_enhanced_metadata(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        logger.info("ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ§Ø¯ÛŒØªØ§...")
        
        metadata_list = []
        
        for i, chunk in enumerate(self.all_chunks):
            chunk_data = {
                "chunk_id": chunk.id,
                "source_file": chunk.source_file,
                "document_title": chunk.document_title,
                "chunk_index": chunk.chunk_index,
                "chunk_type": chunk.chunk_type,
                "document_section": chunk.document_section,
                "article_numbers": chunk.article_numbers,
                "note_numbers": chunk.note_numbers,
                "has_table": chunk.has_table,
                "has_list": chunk.has_list,
                "chunk_length": len(chunk.text),
                "word_count": len(chunk.text.split()),
                "quality_score": chunk.metadata.get('quality_score', 0),
                "chunk_text": chunk.text,
                "metadata": chunk.metadata,
                "indexed_at": datetime.now().isoformat()
            }
            metadata_list.append(chunk_data)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ§Ø¯ÛŒØªØ§
        metadata_path = self.index_dir / "enhanced_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata_list, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… Ù…ØªØ§Ø¯ÛŒØªØ§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {metadata_path}")
    
    def save_comprehensive_summary(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ø®Ù„Ø§ØµÙ‡ Ø¬Ø§Ù…Ø¹"""
        logger.info("ğŸ“‹ Ø°Ø®ÛŒØ±Ù‡ Ø®Ù„Ø§ØµÙ‡ Ø¬Ø§Ù…Ø¹...")
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡
        total_words = sum(len(chunk.text.split()) for chunk in self.all_chunks)
        total_chars = sum(len(chunk.text) for chunk in self.all_chunks)
        
        chunk_lengths = [len(chunk.text) for chunk in self.all_chunks]
        quality_scores = [chunk.metadata.get('quality_score', 0) for chunk in self.all_chunks]
        
        summary = {
            "indexed_at": datetime.now().isoformat(),
            "total_documents": len(set(chunk.source_file for chunk in self.all_chunks)),
            "total_chunks": len(self.all_chunks),
            "total_words": total_words,
            "total_characters": total_chars,
            "embedding_model": self.embedding_model_name,
            "embedding_dimension": self.embedding_dim,
            "chunking_strategy": "advanced_structure_aware",
            "chunk_size_stats": {
                "min": min(chunk_lengths),
                "max": max(chunk_lengths),
                "avg": sum(chunk_lengths) / len(chunk_lengths),
                "std": np.std(chunk_lengths) if chunk_lengths else 0
            },
            "quality_stats": {
                "min": min(quality_scores) if quality_scores else 0,
                "max": max(quality_scores) if quality_scores else 0,
                "avg": sum(quality_scores) / len(quality_scores) if quality_scores else 0
            },
            "document_details": self._get_document_details()
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø®Ù„Ø§ØµÙ‡
        summary_path = self.index_dir / "comprehensive_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… Ø®Ù„Ø§ØµÙ‡ Ø¬Ø§Ù…Ø¹ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {summary_path}")
        
        return summary
    
    def _get_document_details(self) -> List[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª Ø§Ø³Ù†Ø§Ø¯"""
        doc_details = {}
        
        for chunk in self.all_chunks:
            if chunk.source_file not in doc_details:
                doc_details[chunk.source_file] = {
                    "filename": chunk.source_file,
                    "title": chunk.document_title,
                    "chunk_count": 0,
                    "total_length": 0,
                    "total_words": 0,
                    "chunk_types": {},
                    "avg_quality_score": 0
                }
            
            doc = doc_details[chunk.source_file]
            doc["chunk_count"] += 1
            doc["total_length"] += len(chunk.text)
            doc["total_words"] += len(chunk.text.split())
            doc["chunk_types"][chunk.chunk_type] = doc["chunk_types"].get(chunk.chunk_type, 0) + 1
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª
        for doc_file, doc_data in doc_details.items():
            doc_chunks = [chunk for chunk in self.all_chunks if chunk.source_file == doc_file]
            if doc_chunks:
                avg_quality = sum(chunk.metadata.get('quality_score', 0) for chunk in doc_chunks) / len(doc_chunks)
                doc_data["avg_quality_score"] = round(avg_quality, 3)
        
        return list(doc_details.values())
    
    def create_advanced_index(self):
        """Ù…ØªØ¯ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        print("="*80)
        print("ğŸš€ FAISS Index Creator V3 - Advanced RAG Optimized")
        print("="*80)
        
        try:
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø³Ù†Ø§Ø¯
            self.process_documents_parallel()
            
            # Ø§ÛŒØ¬Ø§Ø¯ embeddingÙ‡Ø§
            self.create_embeddings_batch()
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS
            self.create_optimized_faiss_index()
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ§Ø¯ÛŒØªØ§
            self.save_enhanced_metadata()
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø®Ù„Ø§ØµÙ‡
            summary = self.save_comprehensive_summary()
            
            print("\n" + "="*80)
            print("âœ… Ø§ÛŒØ¬Ø§Ø¯ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø§Ù…Ù„ Ø´Ø¯!")
            print("="*80)
            
            # Ú†Ø§Ù¾ Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ
            self._print_final_summary(summary)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ø§ÛŒÙ†Ø¯Ú©Ø³: {e}")
            raise
    
    def _print_final_summary(self, summary: Dict):
        """Ú†Ø§Ù¾ Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ"""
        print(f"\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ:")
        print(f"   ğŸ“„ Ø§Ø³Ù†Ø§Ø¯ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡: {summary['total_documents']}")
        print(f"   âœ‚ï¸  Ú©Ù„ Ù‚Ø·Ø¹Ø§Øª: {summary['total_chunks']}")
        print(f"   ğŸ“ Ú©Ù„ Ú©Ù„Ù…Ø§Øª: {summary['total_words']:,}")
        print(f"   ğŸ”¤ Ú©Ù„ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§: {summary['total_characters']:,}")
        print(f"   ğŸ“ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…ØªÙˆØ³Ø· Ù‚Ø·Ø¹Ø§Øª: {summary['chunk_size_stats']['avg']:.0f} Ú©Ø§Ø±Ø§Ú©ØªØ±")
        print(f"   â­ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª: {summary['quality_stats']['avg']:.2f}")
        print(f"   ğŸ¤– Ù…Ø¯Ù„ embedding: {summary['embedding_model']}")
        print(f"   ğŸ“ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³: {self.index_dir}")
        
        print(f"\nğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡:")
        for file_path in self.index_dir.glob("*"):
            size_mb = file_path.stat().st_size / (1024 * 1024)
            print(f"   ğŸ“„ {file_path.name} ({size_mb:.2f} MB)")

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    try:
        # Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
        config = {
            "documents_dir": "./documents",
            "index_dir": "./index_v3_advanced",
            "embedding_model": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            "max_chunk_size": 800,
            "min_chunk_size": 150
        }
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø§ÛŒÙ†Ø¯Ú©Ø³
        creator = AdvancedFAISSIndexCreator(**config)
        creator.create_advanced_index()
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡: {e}")
        exit(1)

if __name__ == "__main__":
    main()